{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"null values\", \"parsing\", \"duplicates\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The use of `eval` for parsing is unsafe and can execute arbitrary code; it also fails to handle `null` values, which will raise an error.\",\n",
                "        \"The code does not remove duplicate rows, which can skew analysis and model training.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import pandas as pd\n",
                "\n",
                "\n",
                "def load_data(data_path: Path):\n",
                "    train = pd.read_csv(data_path / \"train.csv\")\n",
                "    test = pd.read_csv(data_path / \"test.csv\")\n",
                "    return train, test\n",
                "\n",
                "\n",
                "def clean_data_incorrectly(df: pd.DataFrame):\n",
                "    \"\"\"Incorrectly handles string-to-list conversion and fails to remove duplicates.\"\"\"\n",
                "\n",
                "    for col in [\"prompt\", \"response_a\", \"response_b\"]:\n",
                "        if col in df.columns:\n",
                "            df[col] = df[col].apply(eval)\n",
                "\n",
                "    return df\n",
                "\n",
                "\n",
                "DATA_PATH = Path(\"/kaggle/input/lmsys-chatbot-arena\")\n",
                "train_df, test_df = load_data(DATA_PATH)\n",
                "\n",
                "try:\n",
                "    train_df_cleaned = clean_data_incorrectly(train_df.copy())\n",
                "    print(f\"Dataframe shape after cleaning: {train_df_cleaned.shape}\")\n",
                "except Exception as e:\n",
                "    print(f\"An error occurred during cleaning: {e}\")\n",
                "\n",
                "print(f\"Original dataframe shape: {train_df.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"battle heatmap\", \"distribution\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The analysis is incomplete as it only visualizes the distribution for `model_a` while ignoring `model_b`.\",\n",
                "        \"The required battle heatmap, which shows head-to-head model performance, was not implemented or generated.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "\n",
                "def plot_model_distribution_flawed(df: pd.DataFrame):\n",
                "    \"\"\"Plots distribution but only for one column and omits the battle heatmap.\"\"\"\n",
                "\n",
                "    model_a_counts = df[\"model_a\"].value_counts()\n",
                "\n",
                "    plt.figure(figsize=(8, 8))\n",
                "    plt.pie(\n",
                "        model_a_counts, labels=model_a_counts.index, autopct=\"%1.1f%%\", startangle=140\n",
                "    )\n",
                "    plt.title(\"Distribution for model_a\")\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "plot_model_distribution_flawed(train_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"feature engineering\", \"data visualization\", \"distribution\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The feature engineering is incomplete; it only calculates the number of turns (`n_turns`) and omits other critical length-based features like character counts and response length differences.\",\n",
                "        \"The distributions of the newly created features were not visualized, failing to provide insight into their characteristics.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "\n",
                "def engineer_length_features_partially(df: pd.DataFrame):\n",
                "    \"\"\"Engineers only a subset of required features and does not visualize them.\"\"\"\n",
                "\n",
                "    try:\n",
                "        df[\"n_turns\"] = df[\"prompt\"].apply(len)\n",
                "        print(\"Engineered 'n_turns' feature.\")\n",
                "\n",
                "        print(\"Feature visualization was not performed.\")\n",
                "    except TypeError:\n",
                "        print(\"Could not engineer features because 'prompt' column is not a list.\")\n",
                "    return df\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"baseline model\", \"log loss\", \"class imbalance\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"Only the most naive uniform-probability baseline was implemented; the more informative mean-based baseline was omitted.\",\n",
                "        \"The function generates predictions but fails to calculate the `log_loss` score, so the baseline's performance is never actually measured.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from sklearn.metrics import log_loss\n",
                "\n",
                "\n",
                "def evaluate_naive_baseline_incorrectly(df: pd.DataFrame, targets: list):\n",
                "    \"\"\"Calculates a naive baseline but fails to implement the better mean-based one.\"\"\"\n",
                "\n",
                "    y_pred = [[1 / 3, 1 / 3, 1 / 3]] * len(df)\n",
                "\n",
                "    print(\"Generated uniform predictions, but did not calculate log loss.\")\n",
                "    y_true = df[targets].values\n",
                "    score = log_loss(y_true, y_pred)\n",
                "    print(f\"Uniform Baseline Log Loss: {score:.4f}\")\n",
                "    return None\n",
                "\n",
                "\n",
                "TARGETS = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
                "evaluate_naive_baseline_incorrectly(train_df, TARGETS)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"decision tree\", \"overfitting\", \"cross-validation\", \"dataset\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The model was trained on the entire dataset without cross-validation, making it impossible to get a robust measure of performance and check for overfitting.\",\n",
                "        \"The decision tree was not visualized, which is a key step for interpreting the model and understanding which features are most important.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "\n",
                "\n",
                "def train_decision_tree_flawed(df: pd.DataFrame, features: list, targets: list):\n",
                "    \"\"\"Trains a Decision Tree but omits CV and visualization.\"\"\"\n",
                "\n",
                "    X = df[features]\n",
                "    y = df[targets]\n",
                "\n",
                "    model = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
                "    model.fit(X, y)\n",
                "\n",
                "    print(\"Decision tree trained on the full dataset, but not evaluated or visualized.\")\n",
                "\n",
                "    return model\n",
                "\n",
                "\n",
                "features = [\"n_turns\", \"prompt_len\", \"response_a_len\", \"response_b_len\", \"len_diff\"]\n",
                "train_df_featured = pd.DataFrame(\n",
                "    columns=features, data=np.random.rand(100, len(features))\n",
                ")\n",
                "train_df_featured[TARGETS] = pd.DataFrame(np.random.randint(0, 2, size=(100, 3)))\n",
                "dt_model = train_decision_tree_flawed(train_df_featured, features, TARGETS)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"topic modeling\", \"vectorization\", \"stop words\", \"dimensionality reduction\", \"UMAP\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"BERTopic was configured with a basic `CountVectorizer` that does not remove English stop words, which will likely result in uninformative topics.\",\n",
                "        \"A custom UMAP model was not configured and passed to BERTopic, which is a missed opportunity to tune the dimensionality reduction step for better topic separation.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from bertopic import BERTopic\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "\n",
                "\n",
                "def run_bertopic_with_bad_vectorizer(prompts: pd.Series):\n",
                "    \"\"\"Runs BERTopic with a suboptimal vectorizer and without reducing dimensionality properly.\"\"\"\n",
                "\n",
                "    vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=None)\n",
                "\n",
                "    topic_model = BERTopic(\n",
                "        vectorizer_model=vectorizer_model,\n",
                "        verbose=False,\n",
                "    )\n",
                "\n",
                "    print(\"BERTopic configured with a suboptimal vectorizer and default UMAP.\")\n",
                "\n",
                "    return topic_model\n",
                "\n",
                "\n",
                "prompts = train_df[\"prompt\"].explode()\n",
                "topic_model = run_bertopic_with_bad_vectorizer(prompts)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
