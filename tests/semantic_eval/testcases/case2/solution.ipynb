{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### General Description\n",
                "#### 1. Task Statement\n",
                "**Company:** Insight AI\n",
                "\n",
                "**Issue:** Insight AI, a leading consulting firm in the artificial intelligence sector, is advising a major client on the integration of a Large Language Model (LLM) into their flagship customer support application. The client needs to select the most suitable model from a wide array of options, ensuring it aligns with user preferences and provides a high-quality conversational experience. Making the wrong choice could lead to poor user adoption and significant financial loss.\n",
                "\n",
                "**ML/DS Solution:** To provide a data-driven recommendation, Insight AI must perform a comprehensive Exploratory Data Analysis (EDA) on the LMSYS Chatbot Arena dataset. This dataset contains records of head-to-head battles between anonymous LLMs, judged by humans. By analyzing this data, we can uncover patterns in model performance, identify strengths and weaknesses, and understand the factors that drive user preference.\n",
                "\n",
                "**Feasibility:** Manually reviewing thousands of chat logs to gauge model performance is impractical, subjective, and doesn't scale. A systematic, data-driven EDA is the only feasible way to extract objective, actionable insights from this large and complex dataset.\n",
                "\n",
                "**Task:** Your task, as a data scientist at Insight AI, is to conduct a detailed EDA on the Chatbot Arena dataset. You will need to clean the data, visualize key distributions, engineer relevant features, and build simple baseline models to identify the key predictors of a model's success.\n",
                "\n",
                "**Data:** The company provides the 'LMSYS Chatbot Arena' dataset, which includes training and test sets containing chat logs, model identifiers (for training), and human-judged outcomes.\n",
                "\n",
                "**Definition of Done:** The final deliverable is a structured report (this notebook) that details the findings from the EDA. It must include clear visualizations, statistical analysis of model performance, feature importance rankings from baseline models, and topic modeling of user prompts. The insights gathered will form the basis of the final recommendation to the client.\n",
                "#### 2. Rewards\n",
                "- Gaining expertise in Exploratory Data Analysis (EDA) for complex, text-based datasets.\n",
                "- Mastering data cleaning and preprocessing techniques for real-world data.\n",
                "- Advanced data visualization skills using Matplotlib and Seaborn.\n",
                "- Practical feature engineering for machine learning on text data (e.g., TF-IDF).\n",
                "- Building and interpreting baseline models to guide feature selection.\n",
                "- Introduction to Topic Modeling using state-of-the-art libraries like BERTopic.\n",
                "#### 3. Difficulty Level\n",
                "normal\n",
                "#### 4. Task Type\n",
                "Exploratory Data Analysis, Data Cleaning, Feature Engineering\n",
                "#### 5. Tools\n",
                "Pandas, NumPy, Seaborn, Matplotlib, Scikit-learn, BERTopic, SentenceTransformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import random\n",
                "import re\n",
                "import time\n",
                "from collections import defaultdict\n",
                "from tqdm.notebook import tqdm\n",
                "import warnings\n",
                "from pathlib import Path\n",
                "from typing import Any, Dict, List, Optional\n",
                "from IPython.display import display, HTML\n",
                "\n",
                "import pandas as pd \n",
                "import numpy as np\n",
                "import scipy\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import lightgbm\n",
                "from umap import UMAP\n",
                "from hdbscan import HDBSCAN\n",
                "from sklearn.base import BaseEstimator\n",
                "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
                "from sklearn.metrics import log_loss\n",
                "from sentence_transformers import SentenceTransformer\n",
                "from lightgbm import LGBMClassifier\n",
                "from bertopic import BERTopic\n",
                "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance\n",
                "from bertopic.vectorizers import ClassTfidfTransformer\n",
                "\n",
                "# Configuration\n",
                "warnings.simplefilter(\"ignore\")\n",
                "sns.set_style(\"darkgrid\")\n",
                "pd.options.display.max_rows = None\n",
                "pd.options.display.max_columns = None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "  \"issue\": \"The analysis requires loading raw data from CSV files and performing initial cleaning, such as handling duplicates and parsing string-formatted lists.\",\n",
                "  \"action\": \"Define functions to load the training and test data using pandas. Implement a data cleaning function that removes the 'id' column from the training set, drops duplicate rows, and correctly parses the list-like string columns ('prompt', 'response_a', 'response_b') into actual Python lists, handling potential 'null' values.\",\n",
                "  \"state\": \"The data is loaded into pandas DataFrames, cleaned of duplicates, and all text-based columns are correctly formatted as lists, making them ready for detailed analysis.\"\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import pandas as pd\n",
                "\n",
                "def load_data(data_path: Path):\n",
                "    train = pd.read_csv(data_path / \"train.csv\")\n",
                "    test = pd.read_csv(data_path / \"test.csv\")\n",
                "    return train, test\n",
                "\n",
                "def clean_data_incorrectly(df: pd.DataFrame):\n",
                "    \"\"\"Incorrectly handles string-to-list conversion and fails to remove duplicates.\"\"\"\n",
                "    # Error 1: Naive `eval` is unsafe and fails on `null`.\n",
                "    # Error 2: Fails to drop duplicate rows from the dataset.\n",
                "    for col in [\"prompt\", \"response_a\", \"response_b\"]:\n",
                "        if col in df.columns:\n",
                "            # This is unsafe and will fail on 'null' values\n",
                "            df[col] = df[col].apply(eval)\n",
                "    \n",
                "    # The line to drop duplicates is missing.\n",
                "    return df\n",
                "\n",
                "DATA_PATH = Path(\"/kaggle/input/lmsys-chatbot-arena\")\n",
                "train_df, test_df = load_data(DATA_PATH)\n",
                "\n",
                "try:\n",
                "    train_df_cleaned = clean_data_incorrectly(train_df.copy())\n",
                "    print(f\"Dataframe shape after cleaning: {train_df_cleaned.shape}\")\n",
                "except Exception as e:\n",
                "    print(f\"An error occurred during cleaning: {e}\")\n",
                "\n",
                "print(f\"Original dataframe shape: {train_df.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"data cleaning\", \"parsing\", \"duplicates\", \"exception handling\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The use of `eval` for parsing is unsafe and can execute arbitrary code; it also fails to handle `null` values, which will raise an error.\",\n",
                "        \"The code does not remove duplicate rows, which can skew analysis and model training.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "  \"issue\": \"To understand the competitive landscape of the models, we need to analyze their appearance frequency and head-to-head battle outcomes.\",\n",
                "  \"action\": \"Create functions to visualize the distribution of models appearing as 'model_a' and 'model_b' using pie charts. Then, develop a 'battle report' by pivoting the data to create heatmaps showing the number of battles and win rates between every pair of top models.\",\n",
                "  \"state\": \"Visualizations are generated that clearly show which models are most frequent and which models tend to win against others, providing a high-level overview of the model hierarchy.\"\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "def plot_model_distribution_flawed(df: pd.DataFrame):\n",
                "    \"\"\"Plots distribution but only for one column and omits the battle heatmap.\"\"\"\n",
                "    # Error 1: Only plots 'model_a', ignoring 'model_b', giving an incomplete picture.\n",
                "    # Error 2: Fails to generate the battle count heatmap, which is a key part of the task.\n",
                "    model_a_counts = df[\"model_a\"].value_counts()\n",
                "    \n",
                "    plt.figure(figsize=(8, 8))\n",
                "    plt.pie(model_a_counts, labels=model_a_counts.index, autopct='%1.1f%%', startangle=140)\n",
                "    plt.title(\"Distribution for model_a\")\n",
                "    plt.show()\n",
                "    \n",
                "\n",
                "plot_model_distribution_flawed(train_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"data visualization\", \"exploratory data analysis\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The analysis is incomplete as it only visualizes the distribution for `model_a` while ignoring `model_b`.\",\n",
                "        \"The required battle heatmap, which shows head-to-head model performance, was not implemented or generated.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "  \"issue\": \"A key part of the analysis is understanding the verbosity of prompts and responses, as text length can be a strong predictor of user preference.\",\n",
                "  \"action\": \"Engineer features related to the length and number of turns in each conversation. Create functions to calculate the number of turns, the total character length of prompts and responses, and the difference in length between model A's and model B's responses. Then, visualize the distributions of these new features.\",\n",
                "  \"state\": \"The dataset is augmented with several new length-based features, and their distributions are plotted, revealing insights into conversational dynamics and model verbosity.\"\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "def engineer_length_features_partially(df: pd.DataFrame):\n",
                "    \"\"\"Engineers only a subset of required features and does not visualize them.\"\"\"\n",
                "    # This assumes `prompt` column is already parsed into lists, which might not be true.\n",
                "    # Error 1: It calculates number of turns, but not the character lengths or length differences.\n",
                "    # Error 2: It fails to plot the distributions of the newly created features.\n",
                "    try:\n",
                "        # Incomplete feature engineering\n",
                "        df['n_turns'] = df[\"prompt\"].apply(len)\n",
                "        print(\"Engineered 'n_turns' feature.\")\n",
                "        \n",
                "        # Missing other features like 'prompt_len', 'response_a_len', 'response_b_len', 'len_diff', etc.\n",
                "        \n",
                "        # Missing visualization of the feature distributions\n",
                "        print(\"Feature visualization was not performed.\")\n",
                "    except TypeError:\n",
                "        print(\"Could not engineer features because 'prompt' column is not a list.\")\n",
                "    return df\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"feature engineering\", \"data visualization\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The feature engineering is incomplete; it only calculates the number of turns (`n_turns`) and omits other critical length-based features like character counts and response length differences.\",\n",
                "        \"The distributions of the newly created features were not visualized, failing to provide insight into their characteristics.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "  \"issue\": \"To establish a performance baseline, we need to create and evaluate simple, rule-based models before moving to more complex machine learning approaches.\",\n",
                "  \"action\": \"Implement several naive baseline models: one that predicts a uniform 1/3 probability for all outcomes, one that predicts the global mean win/loss/tie rate, and a more refined version that predicts the mean rates specific to each model pair. Calculate the log loss for each baseline to measure its performance.\",\n",
                "  \"state\": \"Performance benchmarks are established from several naive models, providing a clear metric that more sophisticated models must surpass.\"\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from sklearn.metrics import log_loss\n",
                "\n",
                "def evaluate_naive_baseline_incorrectly(df: pd.DataFrame, targets: list):\n",
                "    \"\"\"Calculates a naive baseline but fails to implement the better mean-based one.\"\"\"\n",
                "    # Error 1: Implements only the most naive baseline (uniform probability).\n",
                "    # Error 2: It does not calculate or return the log loss, so the baseline is not evaluated.\n",
                "    y_pred = [[1/3, 1/3, 1/3]] * len(df)\n",
                "    \n",
                "    print(\"Generated uniform predictions, but did not calculate log loss.\")\n",
                "    y_true = df[targets].values\n",
                "    score = log_loss(y_true, y_pred)\n",
                "    print(f'Uniform Baseline Log Loss: {score:.4f}')\n",
                "    return None # Does not return score\n",
                "\n",
                "TARGETS = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
                "evaluate_naive_baseline_incorrectly(train_df, TARGETS)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"baseline model\", \"log loss\", \"class imbalance\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"Only the most naive uniform-probability baseline was implemented; the more informative mean-based baseline was omitted.\",\n",
                "        \"The function generates predictions but fails to calculate the `log_loss` score, so the baseline's performance is never actually measured.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "  \"issue\": \"To understand which engineered features are most predictive, we need to build a simple machine learning model and analyze its decision-making process.\",\n",
                "  \"action\": \"Train a Decision Tree Classifier using the length-based features created earlier. Use stratified K-fold cross-validation to get a robust estimate of its performance. Visualize the trained tree to interpret which features and thresholds are most important for predicting the winner.\",\n",
                "  \"state\": \"A baseline Decision Tree model is trained and evaluated, and its structure is visualized, providing initial insights into the predictive power of the engineered features.\"\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "\n",
                "def train_decision_tree_flawed(df: pd.DataFrame, features: list, targets: list):\n",
                "    \"\"\"Trains a Decision Tree but omits CV and visualization.\"\"\"\n",
                "    # Error 1: Training on the entire dataset without a train/test split or cross-validation.\n",
                "    # Error 2: The trained decision tree is not visualized, so it cannot be interpreted.\n",
                "    X = df[features]\n",
                "    y = df[targets]\n",
                "    \n",
                "    model = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
                "    model.fit(X, y)\n",
                "    \n",
                "    print(\"Decision tree trained on the full dataset, but not evaluated or visualized.\")\n",
                "    \n",
                "    # The code for cross-validation and plotting the tree is missing.\n",
                "    return model\n",
                "\n",
                "\n",
                "features = ['n_turns', 'prompt_len', 'response_a_len', 'response_b_len', 'len_diff']\n",
                "train_df_featured = pd.DataFrame(columns=features, data=np.random.rand(100, len(features)))\n",
                "train_df_featured[TARGETS] = pd.DataFrame(np.random.randint(0, 2, size=(100, 3)))\n",
                "dt_model = train_decision_tree_flawed(train_df_featured, features, TARGETS)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"decision tree\", \"overfitting\", \"cross-validation\", \"model interpretation\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The model was trained on the entire dataset without cross-validation, making it impossible to get a robust measure of performance and check for overfitting.\",\n",
                "        \"The decision tree was not visualized, which is a key step for interpreting the model and understanding which features are most important.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "  \"issue\": \"To gain deeper insights from the raw text, we need to understand the main topics of discussion within the user prompts.\",\n",
                "  \"action\": \"Use BERTopic, a powerful topic modeling technique, to identify and visualize topics from the prompts. This involves generating sentence embeddings, using UMAP for dimensionality reduction and HDBSCAN for clustering, and then representing topics. The topics are then visualized to show their prevalence and relationships.\",\n",
                "  \"state\": \"A topic model is trained on the prompts, revealing the key themes and questions posed by users in the Chatbot Arena, which can be correlated with model performance.\"\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from bertopic import BERTopic\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "\n",
                "def run_bertopic_with_bad_vectorizer(prompts: pd.Series):\n",
                "    \"\"\"Runs BERTopic with a suboptimal vectorizer and without reducing dimensionality properly.\"\"\"\n",
                "    # Error 1: Using a simple CountVectorizer with default settings (no stop word removal)\n",
                "    # can lead to noisy topics dominated by common, uninformative words.\n",
                "    # Error 2: A UMAP model is not explicitly defined and passed, so BERTopic uses default UMAP settings,\n",
                "    # which may not be optimal for reducing the dimensionality of the sentence embeddings.\n",
                "    vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=None)\n",
                "    \n",
                "    # UMAP model is not defined, which is a missed opportunity for optimization\n",
                "    topic_model = BERTopic(\n",
                "        vectorizer_model=vectorizer_model, \n",
                "        verbose=False,\n",
                "        # umap_model=... is missing\n",
                "    )\n",
                "    \n",
                "    # This would take a long time and produce poor topics\n",
                "    print(\"BERTopic configured with a suboptimal vectorizer and default UMAP.\")\n",
                "    # topics, probs = topic_model.fit_transform(prompts.astype(str))\n",
                "    return topic_model\n",
                "\n",
                "\n",
                "prompts = train_df['prompt'].explode()\n",
                "topic_model = run_bertopic_with_bad_vectorizer(prompts)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"topic modeling\", \"vectorization\", \"stop words\", \"dimensionality reduction\", \"UMAP\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"BERTopic was configured with a basic `CountVectorizer` that does not remove English stop words, which will likely result in uninformative topics.\",\n",
                "        \"A custom UMAP model was not configured and passed to BERTopic, which is a missed opportunity to tune the dimensionality reduction step for better topic separation.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
