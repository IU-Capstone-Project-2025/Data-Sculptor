{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"data cleaning\", \"parsing\", \"duplicates\", \"exception handling\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The use of `eval` for parsing is unsafe and can execute arbitrary code; it also fails to handle `null` values, which will raise an error.\",\n",
                "        \"The code does not remove duplicate rows, which can skew analysis and model training.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import pandas as pd\n",
                "\n",
                "def load_data(data_path: Path):\n",
                "    train = pd.read_csv(data_path / \"train.csv\")\n",
                "    test = pd.read_csv(data_path / \"test.csv\")\n",
                "    return train, test\n",
                "\n",
                "def clean_data_incorrectly(df: pd.DataFrame):\n",
                "    \"\"\"Incorrectly handles string-to-list conversion and fails to remove duplicates.\"\"\"\n",
                "    # Error 1: Naive `eval` is unsafe and fails on `null`.\n",
                "    # Error 2: Fails to drop duplicate rows from the dataset.\n",
                "    for col in [\"prompt\", \"response_a\", \"response_b\"]:\n",
                "        if col in df.columns:\n",
                "            # This is unsafe and will fail on 'null' values\n",
                "            df[col] = df[col].apply(eval)\n",
                "    \n",
                "    # The line to drop duplicates is missing.\n",
                "    return df\n",
                "\n",
                "DATA_PATH = Path(\"/kaggle/input/lmsys-chatbot-arena\")\n",
                "train_df, test_df = load_data(DATA_PATH)\n",
                "\n",
                "try:\n",
                "    train_df_cleaned = clean_data_incorrectly(train_df.copy())\n",
                "    print(f\"Dataframe shape after cleaning: {train_df_cleaned.shape}\")\n",
                "except Exception as e:\n",
                "    print(f\"An error occurred during cleaning: {e}\")\n",
                "\n",
                "print(f\"Original dataframe shape: {train_df.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"data visualization\", \"exploratory data analysis\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The analysis is incomplete as it only visualizes the distribution for `model_a` while ignoring `model_b`.\",\n",
                "        \"The required battle heatmap, which shows head-to-head model performance, was not implemented or generated.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "def plot_model_distribution_flawed(df: pd.DataFrame):\n",
                "    \"\"\"Plots distribution but only for one column and omits the battle heatmap.\"\"\"\n",
                "    # Error 1: Only plots 'model_a', ignoring 'model_b', giving an incomplete picture.\n",
                "    # Error 2: Fails to generate the battle count heatmap, which is a key part of the task.\n",
                "    model_a_counts = df[\"model_a\"].value_counts()\n",
                "    \n",
                "    plt.figure(figsize=(8, 8))\n",
                "    plt.pie(model_a_counts, labels=model_a_counts.index, autopct='%1.1f%%', startangle=140)\n",
                "    plt.title(\"Distribution for model_a\")\n",
                "    plt.show()\n",
                "    \n",
                "\n",
                "plot_model_distribution_flawed(train_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"feature engineering\", \"data visualization\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The feature engineering is incomplete; it only calculates the number of turns (`n_turns`) and omits other critical length-based features like character counts and response length differences.\",\n",
                "        \"The distributions of the newly created features were not visualized, failing to provide insight into their characteristics.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "def engineer_length_features_partially(df: pd.DataFrame):\n",
                "    \"\"\"Engineers only a subset of required features and does not visualize them.\"\"\"\n",
                "    # This assumes `prompt` column is already parsed into lists, which might not be true.\n",
                "    # Error 1: It calculates number of turns, but not the character lengths or length differences.\n",
                "    # Error 2: It fails to plot the distributions of the newly created features.\n",
                "    try:\n",
                "        # Incomplete feature engineering\n",
                "        df['n_turns'] = df[\"prompt\"].apply(len)\n",
                "        print(\"Engineered 'n_turns' feature.\")\n",
                "        \n",
                "        # Missing other features like 'prompt_len', 'response_a_len', 'response_b_len', 'len_diff', etc.\n",
                "        \n",
                "        # Missing visualization of the feature distributions\n",
                "        print(\"Feature visualization was not performed.\")\n",
                "    except TypeError:\n",
                "        print(\"Could not engineer features because 'prompt' column is not a list.\")\n",
                "    return df\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"baseline model\", \"log loss\", \"class imbalance\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"Only the most naive uniform-probability baseline was implemented; the more informative mean-based baseline was omitted.\",\n",
                "        \"The function generates predictions but fails to calculate the `log_loss` score, so the baseline's performance is never actually measured.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from sklearn.metrics import log_loss\n",
                "\n",
                "def evaluate_naive_baseline_incorrectly(df: pd.DataFrame, targets: list):\n",
                "    \"\"\"Calculates a naive baseline but fails to implement the better mean-based one.\"\"\"\n",
                "    # Error 1: Implements only the most naive baseline (uniform probability).\n",
                "    # Error 2: It does not calculate or return the log loss, so the baseline is not evaluated.\n",
                "    y_pred = [[1/3, 1/3, 1/3]] * len(df)\n",
                "    \n",
                "    print(\"Generated uniform predictions, but did not calculate log loss.\")\n",
                "    y_true = df[targets].values\n",
                "    score = log_loss(y_true, y_pred)\n",
                "    print(f'Uniform Baseline Log Loss: {score:.4f}')\n",
                "    return None # Does not return score\n",
                "\n",
                "TARGETS = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
                "evaluate_naive_baseline_incorrectly(train_df, TARGETS)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"decision tree\", \"overfitting\", \"cross-validation\", \"model interpretation\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The model was trained on the entire dataset without cross-validation, making it impossible to get a robust measure of performance and check for overfitting.\",\n",
                "        \"The decision tree was not visualized, which is a key step for interpreting the model and understanding which features are most important.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "\n",
                "def train_decision_tree_flawed(df: pd.DataFrame, features: list, targets: list):\n",
                "    \"\"\"Trains a Decision Tree but omits CV and visualization.\"\"\"\n",
                "    # Error 1: Training on the entire dataset without a train/test split or cross-validation.\n",
                "    # Error 2: The trained decision tree is not visualized, so it cannot be interpreted.\n",
                "    X = df[features]\n",
                "    y = df[targets]\n",
                "    \n",
                "    model = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
                "    model.fit(X, y)\n",
                "    \n",
                "    print(\"Decision tree trained on the full dataset, but not evaluated or visualized.\")\n",
                "    \n",
                "    # The code for cross-validation and plotting the tree is missing.\n",
                "    return model\n",
                "\n",
                "\n",
                "features = ['n_turns', 'prompt_len', 'response_a_len', 'response_b_len', 'len_diff']\n",
                "train_df_featured = pd.DataFrame(columns=features, data=np.random.rand(100, len(features)))\n",
                "train_df_featured[TARGETS] = pd.DataFrame(np.random.randint(0, 2, size=(100, 3)))\n",
                "dt_model = train_decision_tree_flawed(train_df_featured, features, TARGETS)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"topic modeling\", \"vectorization\", \"stop words\", \"dimensionality reduction\", \"UMAP\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"BERTopic was configured with a basic `CountVectorizer` that does not remove English stop words, which will likely result in uninformative topics.\",\n",
                "        \"A custom UMAP model was not configured and passed to BERTopic, which is a missed opportunity to tune the dimensionality reduction step for better topic separation.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from bertopic import BERTopic\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "\n",
                "def run_bertopic_with_bad_vectorizer(prompts: pd.Series):\n",
                "    \"\"\"Runs BERTopic with a suboptimal vectorizer and without reducing dimensionality properly.\"\"\"\n",
                "    # Error 1: Using a simple CountVectorizer with default settings (no stop word removal)\n",
                "    # can lead to noisy topics dominated by common, uninformative words.\n",
                "    # Error 2: A UMAP model is not explicitly defined and passed, so BERTopic uses default UMAP settings,\n",
                "    # which may not be optimal for reducing the dimensionality of the sentence embeddings.\n",
                "    vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=None)\n",
                "    \n",
                "    # UMAP model is not defined, which is a missed opportunity for optimization\n",
                "    topic_model = BERTopic(\n",
                "        vectorizer_model=vectorizer_model, \n",
                "        verbose=False,\n",
                "        # umap_model=... is missing\n",
                "    )\n",
                "    \n",
                "    # This would take a long time and produce poor topics\n",
                "    print(\"BERTopic configured with a suboptimal vectorizer and default UMAP.\")\n",
                "    # topics, probs = topic_model.fit_transform(prompts.astype(str))\n",
                "    return topic_model\n",
                "\n",
                "\n",
                "prompts = train_df['prompt'].explode()\n",
                "topic_model = run_bertopic_with_bad_vectorizer(prompts)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
