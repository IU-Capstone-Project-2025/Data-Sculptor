{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"missing values\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The report of missing values is not sorted, making it difficult to identify the most problematic columns at a glance.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_data(path: str):\n",
                "    \"\"\"Load dataset from CSV file.\"\"\"\n",
                "    return pd.read_csv(path)\n",
                "\n",
                "\n",
                "def inspect_missing_values_incorrectly(df: pd.DataFrame):\n",
                "    \"\"\"Return percentage of missing values per column, but unsorted.\"\"\"\n",
                "    missing_info = df.isnull().mean() * 100\n",
                "    return missing_info\n",
                "\n",
                "\n",
                "DATA_PATH = \"/kaggle/input/data-science-jobs/data_science_job.csv\"\n",
                "\n",
                "df = load_data(DATA_PATH)\n",
                "missing_vals = inspect_missing_values_incorrectly(df)\n",
                "print(\"Missing Value Percentages (unsorted):\")\n",
                "print(missing_vals.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"train-test split\", \"reproducibility\", \"random_state\", \"feature scaling\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"A `random_state` was not provided to `train_test_split`, which means the data split will be different every time, harming reproducibility.\",\n",
                "        \"The features were not scaled before splitting. Since KNN Imputer is a distance-based algorithm, failing to scale features can lead to biased imputation.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def prepare_and_split_data_incorrectly(\n",
                "    df: pd.DataFrame, feature_col: str, target_col: str, test_size: float = 0.2\n",
                "):\n",
                "    \"\"\"Selects features and target, but splits data without a random_state and doesn't scale features.\"\"\"\n",
                "    X = df[[feature_col]]\n",
                "    y = df[target_col]\n",
                "\n",
                "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
                "        X, y, test_size=test_size\n",
                "    )\n",
                "\n",
                "    return X_train, X_test, y_train, y_test\n",
                "\n",
                "\n",
                "FEATURE = \"training_hours\"\n",
                "TARGET = \"target\"\n",
                "\n",
                "X_train, X_test, y_train, y_test = prepare_and_split_data_incorrectly(\n",
                "    df, FEATURE, TARGET\n",
                ")\n",
                "\n",
                "print(f\"Shape of X_train: {X_train.shape}\")\n",
                "print(f\"Shape of X_test: {X_test.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"data leakage\", \"imputation\", \"fit transform\", \"preprocessing\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The imputer was re-fit on the test data by calling `fit_transform` instead of just `transform`. This is a classic form of data leakage, where information from the test set unfairly influences the preprocessing pipeline, leading to overly optimistic and invalid performance metrics.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def apply_knn_imputation_incorrectly(\n",
                "    X_train: pd.DataFrame, X_test: pd.DataFrame, n_neighbors: int = 5\n",
                "):\n",
                "    \"\"\"Applies KNN imputation but incorrectly fits on the test set, causing data leakage.\"\"\"\n",
                "    imputer = sklearn.impute.KNNImputer(n_neighbors=n_neighbors)\n",
                "\n",
                "    X_train_imputed = imputer.fit_transform(X_train)\n",
                "\n",
                "    X_test_imputed = imputer.fit_transform(X_test)\n",
                "\n",
                "    return X_train_imputed, X_test_imputed\n",
                "\n",
                "\n",
                "X_train_imputed, X_test_imputed = apply_knn_imputation_incorrectly(X_train, X_test)\n",
                "print(f\"Missing values after incorrect imputation: {np.isnan(X_test_imputed).sum()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"overfitting\", \"evaluation metric\", \"test set\", \"training data\",\"generalization\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The model's accuracy was calculated on the training data, not the test data. This is known as in-sample evaluation and fails to measure the model's ability to generalize to new, unseen data, which can hide overfitting.\",\n",
                "        \"Because the model was never evaluated on the test set, there is no reliable measure of its actual performance.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_and_evaluate_incorrectly(\n",
                "    X_train: np.ndarray, y_train: pd.Series, X_test: np.ndarray, y_test: pd.Series\n",
                "):\n",
                "    \"\"\"Trains a logistic regression model but evaluates it on the wrong dataset.\"\"\"\n",
                "    model = sklearn.linear_model.LogisticRegression()\n",
                "    model.fit(X_train, y_train)\n",
                "\n",
                "    y_pred_train = model.predict(X_train)\n",
                "    train_accuracy = sklearn.metrics.accuracy_score(y_train, y_pred_train)\n",
                "    print(f\"Model Accuracy (on TRAIN data): {train_accuracy:.4f}\")\n",
                "\n",
                "    print(\"Model was not evaluated on the test set.\")\n",
                "    return model\n",
                "\n",
                "\n",
                "model = train_and_evaluate_incorrectly(X_train_imputed, y_train, X_test_imputed, y_test)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
