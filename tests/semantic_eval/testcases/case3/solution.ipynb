{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"grayscale\", \"alphanumerical\", \"normalization\", \"training dataset\", \"shuffling\", \"tanh activation\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The image directory is not sorted alphanumerically before loading, which will cause a mismatch between grayscale and color image pairs.\",\n",
                "        \"Images are normalized to the [0, 1] range, but the generator's `tanh` activation outputs values in the [-1, 1] range, creating a mismatch that hinders training.\",\n",
                "        \"The training dataset is not shuffled (`.shuffle()`), which can lead to poor model generalization if the data has a sequential order.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "SIZE = 256\n",
                "def load_images_incorrectly(path: str, file_limit: int) -> np.ndarray:\n",
                "    \"\"\"Loads images but fails to sort them and normalizes them into the wrong range.\"\"\"\n",
                "    images = []\n",
                "    # Error 1: Fails to sort the files alphanumerically, leading to mismatched color/gray pairs.\n",
                "    files = os.listdir(path)\n",
                "    # The correct implementation should be: files = sorted_alphanumeric(os.listdir(path))\n",
                "\n",
                "    for i in tqdm(files):\n",
                "        if len(images) >= file_limit: break\n",
                "        img = cv2.imread(os.path.join(path, i), 1)\n",
                "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
                "        img = cv2.resize(img, (SIZE, SIZE))\n",
                "        # Error 2: Normalizing to [0, 1] instead of [-1, 1]. This creates a mismatch with the generator's tanh activation.\n",
                "        img = img.astype('float32') / 255.0\n",
                "        images.append(keras.preprocessing.image.img_to_array(img))\n",
                "    return np.array(images)\n",
                "\n",
                "def create_datasets_unshuffled(color_images, gray_images, train_size, batch_size):\n",
                "    \"\"\"Creates datasets but fails to shuffle the training data.\"\"\"\n",
                "    # Error 3: The training dataset is not shuffled. This can lead to poor model performance if the data has an inherent order.\n",
                "    train_ds = tf.data.Dataset.from_tensor_slices((gray_images[:train_size], color_images[:train_size])).batch(batch_size)\n",
                "    test_ds = tf.data.Dataset.from_tensor_slices((gray_images[train_size:], color_images[train_size:])).batch(batch_size)\n",
                "    return train_ds, test_ds\n",
                "\n",
                "color_img_array = load_images_incorrectly('../input/landscape-image-colorization/landscape Images/color', 2200)\n",
                "gray_img_array = load_images_incorrectly('../input/landscape-image-colorization/landscape Images/gray', 2200)\n",
                "train_dataset, test_dataset = create_datasets_unshuffled(color_img_array, gray_img_array, 2000, 64)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"data visualization\", \"data integrity\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The visualization function contains a logical error where `dataset.take(1)` is used inside the loop, causing it to display the same first sample multiple times instead of different samples.\"\n",
                "    ]\n",
                "}\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_sample_data(dataset: tf.data.Dataset, num_samples: int = 3):\n",
                "    \"\"\"Visualizes data but plots the same image repeatedly.\"\"\"\n",
                "    # Error: `dataset.take(1)` is called inside the loop, so it always shows the first batch.\n",
                "    for _ in range(num_samples):\n",
                "        for gray_batch, color_batch in dataset.take(1):\n",
                "            plt.figure(figsize=(10, 5))\n",
                "            plt.subplot(1, 2, 1)\n",
                "            plt.title('Grayscale Input')\n",
                "            plt.imshow(gray_batch[0])\n",
                "            plt.axis('off')\n",
                "            plt.subplot(1, 2, 2)\n",
                "            plt.title('Color Ground Truth')\n",
                "            plt.imshow(color_batch[0])\n",
                "            plt.axis('off')\n",
                "            plt.show()\n",
                "\n",
                "visualize_sample_data(train_dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"U-Net\", \"skip connections\", \"image-to-image translation\", \"encoder\", \"feature maps\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The U-Net generator was built without skip connections (the `Concatenate` layer is missing). These connections are critical for image-to-image tasks as they allow the decoder to reuse low-level feature maps from the encoder, which is necessary to preserve spatial detail and produce sharp images.\"\n",
                "    ]\n",
                "}\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "def build_generator_no_skips() -> keras.Model:\n",
                "    \"\"\"Builds a U-Net but omits the crucial skip connections.\"\"\"\n",
                "    inputs = keras.layers.Input(shape=[256, 256, 3])\n",
                "    \n",
                "    down_stack = [\n",
                "        downsample(64, 4, apply_batchnorm=False), downsample(128, 4), downsample(256, 4), downsample(512, 4),\n",
                "        downsample(512, 4), downsample(512, 4), downsample(512, 4), downsample(512, 4)\n",
                "    ]\n",
                "    up_stack = [\n",
                "        upsample(512, 4, apply_dropout=True), upsample(512, 4, apply_dropout=True), upsample(512, 4, apply_dropout=True),\n",
                "        upsample(512, 4), upsample(256, 4), upsample(128, 4), upsample(64, 4)\n",
                "    ]\n",
                "    last = keras.layers.Conv2DTranspose(3, 4, strides=2, padding='same', kernel_initializer=tf.random_normal_initializer(0., 0.02), activation='tanh')\n",
                "\n",
                "    x = inputs\n",
                "    skips = []\n",
                "    for down in down_stack:\n",
                "        x = down(x)\n",
                "        skips.append(x)\n",
                "    \n",
                "    skips = reversed(skips[:-1])\n",
                "    \n",
                "    # Error: The Concatenate layer is missing, so skip connections are not formed.\n",
                "    for up, skip in zip(up_stack, skips):\n",
                "        x = up(x)\n",
                "        \n",
                "    x = last(x)\n",
                "    return keras.Model(inputs=inputs, outputs=x)\n",
                "def downsample(filters: int, size: int, apply_batchnorm: bool = True) -> keras.Sequential:\n",
                "    result = keras.Sequential()\n",
                "    result.add(layers.Conv2D(filters, size, strides=2, padding='same', kernel_initializer='he_normal', use_bias=False))\n",
                "    if apply_batchnorm:\n",
                "        result.add(layers.BatchNormalization())\n",
                "    result.add(layers.LeakyReLU())\n",
                "    return result\n",
                "\n",
                "def upsample(filters: int, size: int, apply_dropout: bool = False) -> keras.Sequential:\n",
                "    result = keras.Sequential()\n",
                "    result.add(layers.Conv2DTranspose(filters, size, strides=2, padding='same', kernel_initializer='he_normal', use_bias=False))\n",
                "    result.add(layers.BatchNormalization())\n",
                "    if apply_dropout:\n",
                "        result.add(layers.Dropout(0.5))\n",
                "    result.add(layers.ReLU())\n",
                "    return result\n",
                "\n",
                "def build_generator() -> keras.Model:\n",
                "    inputs = layers.Input(shape=[256, 256, 3])\n",
                "    down_stack = [\n",
                "        downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n",
                "        downsample(128, 4), # (bs, 64, 64, 128)\n",
                "        downsample(256, 4), # (bs, 32, 32, 256)\n",
                "        downsample(512, 4), # (bs, 16, 16, 512)\n",
                "        downsample(512, 4), # (bs, 8, 8, 512)\n",
                "        downsample(512, 4), # (bs, 4, 4, 512)\n",
                "        downsample(512, 4), # (bs, 2, 2, 512)\n",
                "        downsample(512, 4), # (bs, 1, 1, 512)\n",
                "    ]\n",
                "    up_stack = [\n",
                "        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
                "        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
                "        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
                "        upsample(512, 4), # (bs, 16, 16, 1024)\n",
                "        upsample(256, 4), # (bs, 32, 32, 512)\n",
                "        upsample(128, 4), # (bs, 64, 64, 256)\n",
                "        upsample(64, 4), # (bs, 128, 128, 128)\n",
                "    ]\n",
                "    initializer = tf.random_normal_initializer(0., 0.02)\n",
                "    last = layers.Conv2DTranspose(3, 4, strides=2, padding='same', kernel_initializer=initializer, activation='tanh')\n",
                "    x = inputs\n",
                "    skips = []\n",
                "    for down in down_stack:\n",
                "        x = down(x)\n",
                "        skips.append(x)\n",
                "    skips = reversed(skips[:-1])\n",
                "    for up, skip in zip(up_stack, skips):\n",
                "        x = up(x)\n",
                "        x = layers.Concatenate()([x, skip])\n",
                "    x = last(x)\n",
                "    return keras.Model(inputs=inputs, outputs=x)\n",
                "\n",
                "def build_discriminator() -> keras.Model:\n",
                "    initializer = tf.random_normal_initializer(0., 0.02)\n",
                "    inp = layers.Input(shape=[256, 256, 3], name='input_image')\n",
                "    tar = layers.Input(shape=[256, 256, 3], name='target_image')\n",
                "    x = layers.concatenate([inp, tar])\n",
                "    down1 = downsample(64, 4, False)(x)\n",
                "    down2 = downsample(128, 4)(down1)\n",
                "    down3 = downsample(256, 4)(down2)\n",
                "    zero_pad1 = layers.ZeroPadding2D()(down3)\n",
                "    conv = layers.Conv2D(512, 4, strides=1, kernel_initializer=initializer, use_bias=False)(zero_pad1)\n",
                "    batchnorm1 = layers.BatchNormalization()(conv)\n",
                "    leaky_relu = layers.LeakyReLU()(batchnorm1)\n",
                "    zero_pad2 = layers.ZeroPadding2D()(leaky_relu)\n",
                "    last = layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(zero_pad2)\n",
                "    return keras.Model(inputs=[inp, tar], outputs=last)\n",
                "\n",
                "generator = build_generator()\n",
                "discriminator = build_discriminator()\n",
                "generator.summary()\n",
                "discriminator.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"GAN loss\", \"discriminator\", \"generator\", \"L1 loss\", \"L2 loss\", \"Mean Squared Error\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"In the discriminator loss, the labels for real and generated images are swapped. The model is trained to predict 0 for real images and 1 for fakes, which is the opposite of the desired behavior.\",\n",
                "        \"The generator's reconstruction loss uses L2 (Mean Squared Error) instead of the L1 (Mean Absolute Error) loss. L1 loss is often preferred for image generation as it encourages less blurring.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "LAMBDA = 100\n",
                "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
                "\n",
                "def discriminator_loss_incorrect(disc_real_output, disc_generated_output):\n",
                "    \"\"\"Discriminator loss with incorrect labels.\"\"\"\n",
                "    # Error: The labels are swapped. It encourages the discriminator to label real images as fake and vice versa.\n",
                "    real_loss = loss_object(tf.zeros_like(disc_real_output), disc_real_output) # Should be tf.ones_like\n",
                "    generated_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output) # Should be tf.zeros_like\n",
                "    return real_loss + generated_loss\n",
                "\n",
                "def generator_loss_incorrect(disc_generated_output, gen_output, target):\n",
                "    \"\"\"Generator loss uses L2 (MSE) loss instead of the paper's recommended L1 (MAE) loss.\"\"\"\n",
                "    # Error: Using L2 loss can lead to blurrier images compared to L1 loss, which is generally preferred for image-to-image tasks.\n",
                "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
                "    l2_loss = tf.reduce_mean(tf.square(target - gen_output))\n",
                "    total_gen_loss = gan_loss + (LAMBDA * l2_loss)\n",
                "    return total_gen_loss, gan_loss, l2_loss\n",
                "\n",
                "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
                "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"training loop\", \"discriminator\", \"gradient descent\", \"optimizers\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The training step is fundamentally flawed because the discriminator is never shown real images (`disc_real_output` is missing), so it cannot learn to distinguish real from fake.\",\n",
                "        \"The calculated gradients are applied to the wrong models: the generator's gradients are used to update the discriminator, and vice-versa, which will prevent the GAN from training correctly.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@tf.function\n",
                "def train_step(input_image, target, epoch):\n",
                "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
                "      gen_output = generator(input_image, training=True)\n",
                "      \n",
                "      # Error 1: The discriminator is only shown the generated (fake) images, not the real ones.\n",
                "      # It needs to see both to learn to differentiate them.\n",
                "      disc_real_output = None # This is missing\n",
                "      disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
                "\n",
                "      gen_total_loss, _, _ = generator_loss_incorrect(disc_generated_output, gen_output, target)\n",
                "      # The discriminator loss function call will fail without the real output.\n",
                "      # disc_loss = discriminator_loss_incorrect(disc_real_output, disc_generated_output) \n",
                "\n",
                "      # Error 2: The generator's gradients are applied to the discriminator's weights, and vice-versa.\n",
                "      generator_gradients = gen_tape.gradient(gen_total_loss, generator.trainable_variables)\n",
                "      # disc_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
                "      \n",
                "      # This will cause the models to train on the wrong objectives\n",
                "      discriminator_optimizer.apply_gradients(zip(generator_gradients, discriminator.trainable_variables))\n",
                "      # generator_optimizer.apply_gradients(zip(disc_gradients, generator.trainable_variables))\n",
                "\n",
                "def fit(train_ds: tf.data.Dataset, epochs: int):\n",
                "    for epoch in range(epochs):\n",
                "        start = time.time()\n",
                "        print(f\"Epoch: {epoch + 1}/{epochs}\")\n",
                "        \n",
                "        for n, (input_image, target) in tqdm(enumerate(train_ds), total=len(list(train_ds.as_numpy_iterator()))):\n",
                "            train_step(input_image, target)\n",
                "        \n",
                "        print(f'Time taken for epoch {epoch + 1} is {time.time()-start:.2f} sec\\n')\n",
                "\n",
                "EPOCHS = 10\n",
                "fit(train_dataset, epochs=EPOCHS)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"inference\", \"training mode\", \"batch normalization\", \"dropout\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The model was called with `training=True` during evaluation. This is incorrect because layers like Dropout and BatchNormalization behave differently in training and inference modes, potentially leading to inconsistent or non-deterministic results when generating images.\"\n",
                "    ]\n",
                "}\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_images(model, test_input, tar):\n",
                "    \"\"\"Generates images, but uses the model in training mode.\"\"\"\n",
                "    # Error: `training=True` is used for prediction. This is incorrect as it can lead to non-deterministic\n",
                "    # outputs if layers like BatchNormalization or Dropout behave differently during inference.\n",
                "    prediction = model(test_input, training=True)\n",
                "\n",
                "    plt.figure(figsize=(15, 15))\n",
                "    display_list = [test_input[0], tar[0], prediction[0]]\n",
                "    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
                "    for i in range(3):\n",
                "        plt.subplot(1, 3, i+1)\n",
                "        plt.title(title[i])\n",
                "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
                "        plt.axis('off')\n",
                "    plt.show()\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
