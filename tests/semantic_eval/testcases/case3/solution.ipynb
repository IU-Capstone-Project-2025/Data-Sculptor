{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"grayscale\", \"alphanumerical\", \"normalization\", \"training dataset\", \"shuffling\", \"tanh activation\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The image directory is not sorted alphanumerically before loading, which will cause a mismatch between grayscale and color image pairs.\",\n",
                "        \"Images are normalized to the [0, 1] range, but the generator's `tanh` activation outputs values in the [-1, 1] range, creating a mismatch that hinders training.\",\n",
                "        \"The training dataset is not shuffled (`.shuffle()`), which can lead to poor model generalization if the data has a sequential order.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "SIZE = 256\n",
                "\n",
                "\n",
                "def load_images_incorrectly(path: str, file_limit: int) -> np.ndarray:\n",
                "    \"\"\"Loads images but fails to sort them and normalizes them into the wrong range.\"\"\"\n",
                "    images = []\n",
                "\n",
                "    files = os.listdir(path)\n",
                "\n",
                "    for i in tqdm(files):\n",
                "        if len(images) >= file_limit:\n",
                "            break\n",
                "        img = cv2.imread(os.path.join(path, i), 1)\n",
                "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
                "        img = cv2.resize(img, (SIZE, SIZE))\n",
                "\n",
                "        img = img.astype(\"float32\") / 255.0\n",
                "        images.append(keras.preprocessing.image.img_to_array(img))\n",
                "    return np.array(images)\n",
                "\n",
                "\n",
                "def create_datasets_unshuffled(color_images, gray_images, train_size, batch_size):\n",
                "    \"\"\"Creates datasets but fails to shuffle the training data.\"\"\"\n",
                "\n",
                "    train_ds = tf.data.Dataset.from_tensor_slices(\n",
                "        (gray_images[:train_size], color_images[:train_size])\n",
                "    ).batch(batch_size)\n",
                "    test_ds = tf.data.Dataset.from_tensor_slices(\n",
                "        (gray_images[train_size:], color_images[train_size:])\n",
                "    ).batch(batch_size)\n",
                "    return train_ds, test_ds\n",
                "\n",
                "\n",
                "color_img_array = load_images_incorrectly(\n",
                "    \"../input/landscape-image-colorization/landscape Images/color\", 2200\n",
                ")\n",
                "gray_img_array = load_images_incorrectly(\n",
                "    \"../input/landscape-image-colorization/landscape Images/gray\", 2200\n",
                ")\n",
                "train_dataset, test_dataset = create_datasets_unshuffled(\n",
                "    color_img_array, gray_img_array, 2000, 64\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"data visualization\", \"data integrity\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The visualization function contains a logical error where `dataset.take(1)` is used inside the loop, causing it to display the same first sample multiple times instead of different samples.\"\n",
                "    ]\n",
                "}\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_sample_data(dataset: tf.data.Dataset, num_samples: int = 3):\n",
                "    \"\"\"Visualizes data but plots the same image repeatedly.\"\"\"\n",
                "\n",
                "    for _ in range(num_samples):\n",
                "        for gray_batch, color_batch in dataset.take(1):\n",
                "            plt.figure(figsize=(10, 5))\n",
                "            plt.subplot(1, 2, 1)\n",
                "            plt.title(\"Grayscale Input\")\n",
                "            plt.imshow(gray_batch[0])\n",
                "            plt.axis(\"off\")\n",
                "            plt.subplot(1, 2, 2)\n",
                "            plt.title(\"Color Ground Truth\")\n",
                "            plt.imshow(color_batch[0])\n",
                "            plt.axis(\"off\")\n",
                "            plt.show()\n",
                "\n",
                "\n",
                "visualize_sample_data(train_dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"U-Net\", \"skip connections\", \"image-to-image translation\", \"encoder\", \"feature maps\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The U-Net generator was built without skip connections (the `Concatenate` layer is missing). These connections are critical for image-to-image tasks as they allow the decoder to reuse low-level feature maps from the encoder, which is necessary to preserve spatial detail and produce sharp images.\"\n",
                "    ]\n",
                "}\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_generator_no_skips() -> keras.Model:\n",
                "    \"\"\"Builds a U-Net but omits the crucial skip connections.\"\"\"\n",
                "    inputs = keras.layers.Input(shape=[256, 256, 3])\n",
                "\n",
                "    down_stack = [\n",
                "        downsample(64, 4, apply_batchnorm=False),\n",
                "        downsample(128, 4),\n",
                "        downsample(256, 4),\n",
                "        downsample(512, 4),\n",
                "        downsample(512, 4),\n",
                "        downsample(512, 4),\n",
                "        downsample(512, 4),\n",
                "        downsample(512, 4),\n",
                "    ]\n",
                "    up_stack = [\n",
                "        upsample(512, 4, apply_dropout=True),\n",
                "        upsample(512, 4, apply_dropout=True),\n",
                "        upsample(512, 4, apply_dropout=True),\n",
                "        upsample(512, 4),\n",
                "        upsample(256, 4),\n",
                "        upsample(128, 4),\n",
                "        upsample(64, 4),\n",
                "    ]\n",
                "    last = keras.layers.Conv2DTranspose(\n",
                "        3,\n",
                "        4,\n",
                "        strides=2,\n",
                "        padding=\"same\",\n",
                "        kernel_initializer=tf.random_normal_initializer(0.0, 0.02),\n",
                "        activation=\"tanh\",\n",
                "    )\n",
                "\n",
                "    x = inputs\n",
                "    skips = []\n",
                "    for down in down_stack:\n",
                "        x = down(x)\n",
                "        skips.append(x)\n",
                "\n",
                "    skips = reversed(skips[:-1])\n",
                "\n",
                "    for up, skip in zip(up_stack, skips):\n",
                "        x = up(x)\n",
                "\n",
                "    x = last(x)\n",
                "    return keras.Model(inputs=inputs, outputs=x)\n",
                "\n",
                "\n",
                "def downsample(\n",
                "    filters: int, size: int, apply_batchnorm: bool = True\n",
                ") -> keras.Sequential:\n",
                "    result = keras.Sequential()\n",
                "    result.add(\n",
                "        layers.Conv2D(\n",
                "            filters,\n",
                "            size,\n",
                "            strides=2,\n",
                "            padding=\"same\",\n",
                "            kernel_initializer=\"he_normal\",\n",
                "            use_bias=False,\n",
                "        )\n",
                "    )\n",
                "    if apply_batchnorm:\n",
                "        result.add(layers.BatchNormalization())\n",
                "    result.add(layers.LeakyReLU())\n",
                "    return result\n",
                "\n",
                "\n",
                "def upsample(filters: int, size: int, apply_dropout: bool = False) -> keras.Sequential:\n",
                "    result = keras.Sequential()\n",
                "    result.add(\n",
                "        layers.Conv2DTranspose(\n",
                "            filters,\n",
                "            size,\n",
                "            strides=2,\n",
                "            padding=\"same\",\n",
                "            kernel_initializer=\"he_normal\",\n",
                "            use_bias=False,\n",
                "        )\n",
                "    )\n",
                "    result.add(layers.BatchNormalization())\n",
                "    if apply_dropout:\n",
                "        result.add(layers.Dropout(0.5))\n",
                "    result.add(layers.ReLU())\n",
                "    return result\n",
                "\n",
                "\n",
                "def build_generator() -> keras.Model:\n",
                "    inputs = layers.Input(shape=[256, 256, 3])\n",
                "    down_stack = [\n",
                "        downsample(64, 4, apply_batchnorm=False),\n",
                "        downsample(128, 4),\n",
                "        downsample(256, 4),\n",
                "        downsample(512, 4),\n",
                "        downsample(512, 4),\n",
                "        downsample(512, 4),\n",
                "        downsample(512, 4),\n",
                "        downsample(512, 4),\n",
                "    ]\n",
                "    up_stack = [\n",
                "        upsample(512, 4, apply_dropout=True),\n",
                "        upsample(512, 4, apply_dropout=True),\n",
                "        upsample(512, 4, apply_dropout=True),\n",
                "        upsample(512, 4),\n",
                "        upsample(256, 4),\n",
                "        upsample(128, 4),\n",
                "        upsample(64, 4),\n",
                "    ]\n",
                "    initializer = tf.random_normal_initializer(0.0, 0.02)\n",
                "    last = layers.Conv2DTranspose(\n",
                "        3,\n",
                "        4,\n",
                "        strides=2,\n",
                "        padding=\"same\",\n",
                "        kernel_initializer=initializer,\n",
                "        activation=\"tanh\",\n",
                "    )\n",
                "    x = inputs\n",
                "    skips = []\n",
                "    for down in down_stack:\n",
                "        x = down(x)\n",
                "        skips.append(x)\n",
                "    skips = reversed(skips[:-1])\n",
                "    for up, skip in zip(up_stack, skips):\n",
                "        x = up(x)\n",
                "        x = layers.Concatenate()([x, skip])\n",
                "    x = last(x)\n",
                "    return keras.Model(inputs=inputs, outputs=x)\n",
                "\n",
                "\n",
                "def build_discriminator() -> keras.Model:\n",
                "    initializer = tf.random_normal_initializer(0.0, 0.02)\n",
                "    inp = layers.Input(shape=[256, 256, 3], name=\"input_image\")\n",
                "    tar = layers.Input(shape=[256, 256, 3], name=\"target_image\")\n",
                "    x = layers.concatenate([inp, tar])\n",
                "    down1 = downsample(64, 4, False)(x)\n",
                "    down2 = downsample(128, 4)(down1)\n",
                "    down3 = downsample(256, 4)(down2)\n",
                "    zero_pad1 = layers.ZeroPadding2D()(down3)\n",
                "    conv = layers.Conv2D(\n",
                "        512, 4, strides=1, kernel_initializer=initializer, use_bias=False\n",
                "    )(zero_pad1)\n",
                "    batchnorm1 = layers.BatchNormalization()(conv)\n",
                "    leaky_relu = layers.LeakyReLU()(batchnorm1)\n",
                "    zero_pad2 = layers.ZeroPadding2D()(leaky_relu)\n",
                "    last = layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(zero_pad2)\n",
                "    return keras.Model(inputs=[inp, tar], outputs=last)\n",
                "\n",
                "\n",
                "generator = build_generator()\n",
                "discriminator = build_discriminator()\n",
                "generator.summary()\n",
                "discriminator.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"GAN loss\", \"discriminator\", \"generator\", \"L1 loss\", \"L2 loss\", \"Mean Squared Error\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"In the discriminator loss, the labels for real and generated images are swapped. The model is trained to predict 0 for real images and 1 for fakes, which is the opposite of the desired behavior.\",\n",
                "        \"The generator's reconstruction loss uses L2 (Mean Squared Error) instead of the L1 (Mean Absolute Error) loss. L1 loss is often preferred for image generation as it encourages less blurring.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "LAMBDA = 100\n",
                "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
                "\n",
                "\n",
                "def discriminator_loss_incorrect(disc_real_output, disc_generated_output):\n",
                "    \"\"\"Discriminator loss with incorrect labels.\"\"\"\n",
                "\n",
                "    real_loss = loss_object(tf.zeros_like(disc_real_output), disc_real_output)\n",
                "    generated_loss = loss_object(\n",
                "        tf.ones_like(disc_generated_output), disc_generated_output\n",
                "    )\n",
                "    return real_loss + generated_loss\n",
                "\n",
                "\n",
                "def generator_loss_incorrect(disc_generated_output, gen_output, target):\n",
                "    \"\"\"Generator loss uses L2 (MSE) loss instead of the paper's recommended L1 (MAE) loss.\"\"\"\n",
                "\n",
                "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
                "    l2_loss = tf.reduce_mean(tf.square(target - gen_output))\n",
                "    total_gen_loss = gan_loss + (LAMBDA * l2_loss)\n",
                "    return total_gen_loss, gan_loss, l2_loss\n",
                "\n",
                "\n",
                "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
                "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"training loop\", \"discriminator\", \"gradient descent\", \"optimizers\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The training step is fundamentally flawed because the discriminator is never shown real images (`disc_real_output` is missing), so it cannot learn to distinguish real from fake.\",\n",
                "        \"The calculated gradients are applied to the wrong models: the generator's gradients are used to update the discriminator, and vice-versa, which will prevent the GAN from training correctly.\"\n",
                "    ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@tf.function\n",
                "def train_step(input_image, target, epoch):\n",
                "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
                "        gen_output = generator(input_image, training=True)\n",
                "\n",
                "        disc_real_output = None\n",
                "        disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
                "\n",
                "        gen_total_loss, _, _ = generator_loss_incorrect(\n",
                "            disc_generated_output, gen_output, target\n",
                "        )\n",
                "\n",
                "        generator_gradients = gen_tape.gradient(\n",
                "            gen_total_loss, generator.trainable_variables\n",
                "        )\n",
                "\n",
                "        discriminator_optimizer.apply_gradients(\n",
                "            zip(generator_gradients, discriminator.trainable_variables)\n",
                "        )\n",
                "\n",
                "\n",
                "def fit(train_ds: tf.data.Dataset, epochs: int):\n",
                "    for epoch in range(epochs):\n",
                "        start = time.time()\n",
                "        print(f\"Epoch: {epoch + 1}/{epochs}\")\n",
                "\n",
                "        for n, (input_image, target) in tqdm(\n",
                "            enumerate(train_ds), total=len(list(train_ds.as_numpy_iterator()))\n",
                "        ):\n",
                "            train_step(input_image, target)\n",
                "\n",
                "        print(f\"Time taken for epoch {epoch + 1} is {time.time() - start:.2f} sec\\n\")\n",
                "\n",
                "\n",
                "EPOCHS = 10\n",
                "fit(train_dataset, epochs=EPOCHS)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "    \"required_ml_terms\": [\"inference\", \"training mode\", \"batch normalization\", \"dropout\"],\n",
                "    \"problems_to_detect\": [\n",
                "        \"The model was called with `training=True` during evaluation. This is incorrect because layers like Dropout and BatchNormalization behave differently in training and inference modes, potentially leading to inconsistent or non-deterministic results when generating images.\"\n",
                "    ]\n",
                "}\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_images(model, test_input, tar):\n",
                "    \"\"\"Generates images, but uses the model in training mode.\"\"\"\n",
                "\n",
                "    prediction = model(test_input, training=True)\n",
                "\n",
                "    plt.figure(figsize=(15, 15))\n",
                "    display_list = [test_input[0], tar[0], prediction[0]]\n",
                "    title = [\"Input Image\", \"Ground Truth\", \"Predicted Image\"]\n",
                "    for i in range(3):\n",
                "        plt.subplot(1, 3, i + 1)\n",
                "        plt.title(title[i])\n",
                "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
                "        plt.axis(\"off\")\n",
                "    plt.show()\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
