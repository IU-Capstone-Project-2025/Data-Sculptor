{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### General Description\n",
                "#### 1. Task Statement\n",
                "**Company:** Artisan Archives\n",
                "\n",
                "**Issue:** Artisan Archives, a company specializing in the digitization and preservation of historical visual media, possesses an enormous collection of black-and-white photographs. The process of manually colorizing these images is incredibly time-consuming, costly, and requires highly skilled digital artists. This manual approach severely limits the company's ability to restore and monetize its vast archive in a timely manner.\n",
                "\n",
                "**ML/DS Solution:** To address this, we can leverage a deep learning technique called Image-to-Image Translation using a Generative Adversarial Network (GAN). Specifically, a pix2pix model can be trained on pairs of color and grayscale images. The model learns the mapping from the grayscale input to the corresponding color output, enabling automated colorization of new images.\n",
                "\n",
                "**Feasibility:** A manual solution is not feasible due to the sheer volume of the archive (millions of images). The cost per image and the time required for manual colorization make it commercially unviable to process the entire collection.\n",
                "\n",
                "**Task:** Artisan Archives has hired you to develop a proof-of-concept machine learning model that can automatically colorize grayscale landscape photographs.\n",
                "\n",
                "**Data:** The company provides the 'Landscape Image Colorization' dataset, which contains pairs of color and grayscale landscape images.\n",
                "\n",
                "**Definition of Done:** The primary goal is to produce visually plausible colorizations. The trained model, after 10 epochs, should generate color images from grayscale inputs that are realistic and artifact-free. Success will be evaluated qualitatively through visual inspection of the output images against their ground-truth counterparts.\n",
                "#### 2. Rewards\n",
                "- Understanding and implementing Generative Adversarial Networks (GANs).\n",
                "- Practical experience with Image-to-Image Translation (pix2pix architecture).\n",
                "- Building and training models in TensorFlow and Keras.\n",
                "- Implementing custom training loops for complex models.\n",
                "- Data preprocessing and augmentation for computer vision tasks.\n",
                "#### 3. Difficulty Level\n",
                "challenging\n",
                "#### 4. Task Type\n",
                "Image Generation, Computer Vision, Generative Adversarial Networks\n",
                "#### 5. Tools\n",
                "TensorFlow, Keras, NumPy, Matplotlib, OpenCV, Scikit-learn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "import keras\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import cv2\n",
                "import os\n",
                "import re\n",
                "import time\n",
                "from tqdm import tqdm\n",
                "from typing import List, Tuple, Any"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "  \"issue\": \"The model requires paired color and grayscale images for training, which must be loaded from disk, preprocessed, and structured into an efficient data pipeline.\",\n",
                "  \"action\": \"Implement functions to load image files from specified directories, sort them alphanumerically to ensure correct pairing, resize them to a uniform dimension (256x256), normalize pixel values to the [-1, 1] range, and then batch them into `tf.data.Dataset` objects for both training and testing sets.\",\n",
                "  \"state\": \"The image data is loaded, preprocessed, and organized into training and testing `tf.data.Dataset` pipelines, ready for consumption by the model.\"\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "SIZE = 256\n",
                "\n",
                "def sorted_alphanumeric(data: List[str]) -> List[str]:\n",
                "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
                "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
                "    return sorted(data, key=alphanum_key)\n",
                "\n",
                "def load_images(path: str, file_limit: int) -> np.ndarray:\n",
                "    images = []\n",
                "    files = os.listdir(path)\n",
                "    files = sorted_alphanumeric(files)\n",
                "    for i in tqdm(files):\n",
                "        if len(images) >= file_limit:\n",
                "            break\n",
                "        img = cv2.imread(os.path.join(path, i), 1)\n",
                "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
                "        img = cv2.resize(img, (SIZE, SIZE))\n",
                "        img = img.astype('float32') / 255.0\n",
                "        images.append(keras.preprocessing.image.img_to_array(img))\n",
                "    return np.array(images)\n",
                "\n",
                "def create_datasets(color_images: np.ndarray, gray_images: np.ndarray, train_size: int, batch_size: int) -> Tuple[tf.data.Dataset, tf.data.Dataset]:\n",
                "    train_color_ds = tf.data.Dataset.from_tensor_slices(color_images[:train_size]).batch(batch_size)\n",
                "    train_gray_ds = tf.data.Dataset.from_tensor_slices(gray_images[:train_size]).batch(batch_size)\n",
                "    \n",
                "    test_color_ds = tf.data.Dataset.from_tensor_slices(color_images[train_size:]).batch(batch_size)\n",
                "    test_gray_ds = tf.data.Dataset.from_tensor_slices(gray_images[train_size:]).batch(batch_size)\n",
                "\n",
                "    train_ds = tf.data.Dataset.zip((train_gray_ds, train_color_ds))\n",
                "    test_ds = tf.data.Dataset.zip((test_gray_ds, test_color_ds))\n",
                "\n",
                "    return train_ds, test_ds\n",
                "\n",
                "# NOTE: The original notebook had hardcoded paths. Update these to your local environment.\n",
                "color_path = '../input/landscape-image-colorization/landscape Images/color'\n",
                "gray_path = '../input/landscape-image-colorization/landscape Images/gray'\n",
                "IMAGE_LIMIT = 2200\n",
                "TRAIN_SPLIT = 2000\n",
                "BATCH_SIZE = 64\n",
                "\n",
                "color_img_array = load_images(color_path, IMAGE_LIMIT)\n",
                "gray_img_array = load_images(gray_path, IMAGE_LIMIT)\n",
                "\n",
                "train_dataset, test_dataset = create_datasets(color_img_array, gray_img_array, TRAIN_SPLIT, BATCH_SIZE)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "  \"issue\": \"Before training, it's crucial to verify that the data has been loaded and paired correctly.\",\n",
                "  \"action\": \"Create a utility function that fetches a few sample pairs from the dataset and displays the grayscale input and its corresponding color ground truth side-by-side.\",\n",
                "  \"state\": \"Visual confirmation is obtained, showing that the input data and target data are correctly aligned and formatted.\"\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_sample_data(dataset: tf.data.Dataset, num_samples: int = 3):\n",
                "    for gray_batch, color_batch in dataset.take(num_samples):\n",
                "        plt.figure(figsize=(10, 5))\n",
                "        \n",
                "        plt.subplot(1, 2, 1)\n",
                "        plt.title('Grayscale Input')\n",
                "        plt.imshow(gray_batch[0])\n",
                "        plt.axis('off')\n",
                "\n",
                "        plt.subplot(1, 2, 2)\n",
                "        plt.title('Color Ground Truth')\n",
                "        plt.imshow(color_batch[0])\n",
                "        plt.axis('off')\n",
                "        \n",
                "        plt.show()\n",
                "\n",
                "visualize_sample_data(train_dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "  \"issue\": \"The core of the pix2pix model requires a Generator and a Discriminator. The Generator must learn to create realistic color images, while the Discriminator must learn to distinguish real color images from fake ones.\",\n",
                "  \"action\": \"Define three functions: `build_generator` creates a U-Net architecture, which is excellent for image-to-image tasks as it preserves spatial information through skip connections. `build_discriminator` creates a PatchGAN classifier, which evaluates realism on patches of the image rather than the whole, promoting sharper outputs. `downsample` and `upsample` utility functions are also created to build these models cleanly.\",\n",
                "  \"state\": \"The architectural blueprints for the Generator and Discriminator are complete, and instances of these models are created and ready for training.\"\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def downsample(filters: int, size: int, apply_batchnorm: bool = True) -> keras.Sequential:\n",
                "    result = keras.Sequential()\n",
                "    result.add(layers.Conv2D(filters, size, strides=2, padding='same', kernel_initializer='he_normal', use_bias=False))\n",
                "    if apply_batchnorm:\n",
                "        result.add(layers.BatchNormalization())\n",
                "    result.add(layers.LeakyReLU())\n",
                "    return result\n",
                "\n",
                "def upsample(filters: int, size: int, apply_dropout: bool = False) -> keras.Sequential:\n",
                "    result = keras.Sequential()\n",
                "    result.add(layers.Conv2DTranspose(filters, size, strides=2, padding='same', kernel_initializer='he_normal', use_bias=False))\n",
                "    result.add(layers.BatchNormalization())\n",
                "    if apply_dropout:\n",
                "        result.add(layers.Dropout(0.5))\n",
                "    result.add(layers.ReLU())\n",
                "    return result\n",
                "\n",
                "def build_generator() -> keras.Model:\n",
                "    inputs = layers.Input(shape=[256, 256, 3])\n",
                "    down_stack = [\n",
                "        downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n",
                "        downsample(128, 4), # (bs, 64, 64, 128)\n",
                "        downsample(256, 4), # (bs, 32, 32, 256)\n",
                "        downsample(512, 4), # (bs, 16, 16, 512)\n",
                "        downsample(512, 4), # (bs, 8, 8, 512)\n",
                "        downsample(512, 4), # (bs, 4, 4, 512)\n",
                "        downsample(512, 4), # (bs, 2, 2, 512)\n",
                "        downsample(512, 4), # (bs, 1, 1, 512)\n",
                "    ]\n",
                "    up_stack = [\n",
                "        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
                "        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
                "        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
                "        upsample(512, 4), # (bs, 16, 16, 1024)\n",
                "        upsample(256, 4), # (bs, 32, 32, 512)\n",
                "        upsample(128, 4), # (bs, 64, 64, 256)\n",
                "        upsample(64, 4), # (bs, 128, 128, 128)\n",
                "    ]\n",
                "    initializer = tf.random_normal_initializer(0., 0.02)\n",
                "    last = layers.Conv2DTranspose(3, 4, strides=2, padding='same', kernel_initializer=initializer, activation='tanh')\n",
                "    x = inputs\n",
                "    skips = []\n",
                "    for down in down_stack:\n",
                "        x = down(x)\n",
                "        skips.append(x)\n",
                "    skips = reversed(skips[:-1])\n",
                "    for up, skip in zip(up_stack, skips):\n",
                "        x = up(x)\n",
                "        x = layers.Concatenate()([x, skip])\n",
                "    x = last(x)\n",
                "    return keras.Model(inputs=inputs, outputs=x)\n",
                "\n",
                "def build_discriminator() -> keras.Model:\n",
                "    initializer = tf.random_normal_initializer(0., 0.02)\n",
                "    inp = layers.Input(shape=[256, 256, 3], name='input_image')\n",
                "    tar = layers.Input(shape=[256, 256, 3], name='target_image')\n",
                "    x = layers.concatenate([inp, tar])\n",
                "    down1 = downsample(64, 4, False)(x)\n",
                "    down2 = downsample(128, 4)(down1)\n",
                "    down3 = downsample(256, 4)(down2)\n",
                "    zero_pad1 = layers.ZeroPadding2D()(down3)\n",
                "    conv = layers.Conv2D(512, 4, strides=1, kernel_initializer=initializer, use_bias=False)(zero_pad1)\n",
                "    batchnorm1 = layers.BatchNormalization()(conv)\n",
                "    leaky_relu = layers.LeakyReLU()(batchnorm1)\n",
                "    zero_pad2 = layers.ZeroPadding2D()(leaky_relu)\n",
                "    last = layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(zero_pad2)\n",
                "    return keras.Model(inputs=[inp, tar], outputs=last)\n",
                "\n",
                "generator = build_generator()\n",
                "discriminator = build_discriminator()\n",
                "generator.summary()\n",
                "discriminator.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "  \"issue\": \"Training a GAN requires a carefully defined set of loss functions and optimizers, as well as a single-step training function that correctly updates both the generator and discriminator.\",\n",
                "  \"action\": \"Define separate loss functions for the generator and discriminator. The discriminator loss penalizes misclassifying real and fake images. The generator loss has two components: a GAN loss to fool the discriminator and an L1 loss to ensure the generated image is structurally similar to the ground truth. An Adam optimizer is created for each model. Finally, the `@tf.function`-decorated `train_step` function is created to perform one step of training: it calculates losses, computes gradients, and applies them to update the model weights.\",\n",
                "  \"state\": \"The complete logic for a single training step, including loss calculations and model updates, is encapsulated and optimized, ready to be called in a loop.\"\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "LAMBDA = 100\n",
                "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
                "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
                "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
                "\n",
                "def generator_loss(disc_generated_output: tf.Tensor, gen_output: tf.Tensor, target: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
                "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
                "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
                "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
                "    return total_gen_loss, gan_loss, l1_loss\n",
                "\n",
                "def discriminator_loss(disc_real_output: tf.Tensor, disc_generated_output: tf.Tensor) -> tf.Tensor:\n",
                "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
                "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
                "    total_disc_loss = real_loss + generated_loss\n",
                "    return total_disc_loss\n",
                "\n",
                "@tf.function\n",
                "def train_step(input_image: tf.Tensor, target: tf.Tensor):\n",
                "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
                "        gen_output = generator(input_image, training=True)\n",
                "        disc_real_output = discriminator([input_image, target], training=True)\n",
                "        disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
                "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
                "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
                "    \n",
                "    generator_gradients = gen_tape.gradient(gen_total_loss, generator.trainable_variables)\n",
                "    discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
                "    \n",
                "    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
                "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "  \"issue\": \"The model must be trained for a set number of epochs over the entire training dataset.\",\n",
                "  \"action\": \"A `fit` function is defined to manage the training loop. It iterates for a specified number of epochs, and in each epoch, it iterates through every batch in the training dataset, calling the `train_step` function for each. It also times each epoch to monitor training speed.\",\n",
                "  \"state\": \"The GAN model is trained on the landscape dataset for 10 epochs. The weights of the generator and discriminator are updated, and the model learns to perform the colorization task.\"\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def fit(train_ds: tf.data.Dataset, epochs: int):\n",
                "    for epoch in range(epochs):\n",
                "        start = time.time()\n",
                "        print(f\"Epoch: {epoch + 1}/{epochs}\")\n",
                "        \n",
                "        for n, (input_image, target) in tqdm(enumerate(train_ds), total=len(list(train_ds.as_numpy_iterator()))):\n",
                "            train_step(input_image, target)\n",
                "        \n",
                "        print(f'Time taken for epoch {epoch + 1} is {time.time()-start:.2f} sec\\n')\n",
                "\n",
                "EPOCHS = 10\n",
                "fit(train_dataset, epochs=EPOCHS)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "  \"issue\": \"After training, the model's performance must be visually assessed to determine the quality of the colorization.\",\n",
                "  \"action\": \"An evaluation function is created that takes the trained generator and a sample from the test set. It generates a colorized image from the grayscale input and then plots the input, the ground truth, and the model's prediction side-by-side for easy comparison.\",\n",
                "  \"state\": \"The qualitative performance of the model is demonstrated through several plotted examples, showing its ability to generate plausible color images from grayscale inputs.\"\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_and_plot_results(model: keras.Model, test_dataset: tf.data.Dataset, num_images: int = 3):\n",
                "    for example_input, example_target in test_dataset.take(num_images):\n",
                "        prediction = model(example_input, training=True)\n",
                "        \n",
                "        plt.figure(figsize=(15, 5))\n",
                "        display_list = [example_input[0], example_target[0], prediction[0]]\n",
                "        title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
                "        \n",
                "        for i in range(3):\n",
                "            plt.subplot(1, 3, i + 1)\n",
                "            plt.title(title[i])\n",
                "            # Clip values to [0, 1] for proper display\n",
                "            image_to_show = np.clip(display_list[i], 0, 1)\n",
                "            plt.imshow(image_to_show)\n",
                "            plt.axis('off')\n",
                "        plt.show()\n",
                "\n",
                "evaluate_and_plot_results(generator, test_dataset)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
