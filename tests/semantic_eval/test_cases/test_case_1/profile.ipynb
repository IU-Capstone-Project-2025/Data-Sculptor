{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### General Description\n",
                "#### 1. Task Statement\n",
                "**Company:** Insight AI\n\n",
                "**Issue:** Insight AI, a leading consulting firm in the artificial intelligence sector, is advising a major client on the integration of a Large Language Model (LLM) into their flagship customer support application. The client needs to select the most suitable model from a wide array of options, ensuring it aligns with user preferences and provides a high-quality conversational experience. Making the wrong choice could lead to poor user adoption and significant financial loss.\n\n",
                "**ML/DS Solution:** To provide a data-driven recommendation, Insight AI must perform a comprehensive Exploratory Data Analysis (EDA) on the LMSYS Chatbot Arena dataset. This dataset contains records of head-to-head battles between anonymous LLMs, judged by humans. By analyzing this data, we can uncover patterns in model performance, identify strengths and weaknesses, and understand the factors that drive user preference.\n\n",
                "**Feasibility:** Manually reviewing thousands of chat logs to gauge model performance is impractical, subjective, and doesn't scale. A systematic, data-driven EDA is the only feasible way to extract objective, actionable insights from this large and complex dataset.\n\n",
                "**Task:** Your task, as a data scientist at Insight AI, is to conduct a detailed EDA on the Chatbot Arena dataset. You will need to clean the data, visualize key distributions, engineer relevant features, and build simple baseline models to identify the key predictors of a model's success.\n\n",
                "**Data:** The company provides the 'LMSYS Chatbot Arena' dataset, which includes training and test sets containing chat logs, model identifiers (for training), and human-judged outcomes.\n\n",
                "**Definition of Done:** The final deliverable is a structured report (this notebook) that details the findings from the EDA. It must include clear visualizations, statistical analysis of model performance, feature importance rankings from baseline models, and topic modeling of user prompts. The insights gathered will form the basis of the final recommendation to the client.\n",
                "#### 2. Rewards\n",
                "- Gaining expertise in Exploratory Data Analysis (EDA) for complex, text-based datasets.\n",
                "- Mastering data cleaning and preprocessing techniques for real-world data.\n",
                "- Advanced data visualization skills using Matplotlib and Seaborn.\n",
                "- Practical feature engineering for machine learning on text data (e.g., TF-IDF).\n",
                "- Building and interpreting baseline models to guide feature selection.\n",
                "- Introduction to Topic Modeling using state-of-the-art libraries like BERTopic.\n",
                "#### 3. Difficulty Level\n",
                "normal\n",
                "#### 4. Task Type\n",
                "Exploratory Data Analysis, Data Cleaning, Feature Engineering\n",
                "#### 5. Tools\n",
                "Pandas, NumPy, Seaborn, Matplotlib, Scikit-learn, BERTopic, SentenceTransformers"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import os\n",
                "import random\n",
                "***REMOVED***\n",
                "import time\n",
                "from collections import defaultdict\n",
                "from tqdm.notebook import tqdm\n",
                "import warnings\n",
                "from pathlib import Path\n",
                "from typing import Any, Dict, List, Optional\n",
                "from IPython.display import display, HTML\n\n",
                "import pandas as pd \n",
                "import numpy as np\n",
                "import scipy\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import lightgbm\n",
                "from umap import UMAP\n",
                "from hdbscan import HDBSCAN\n",
                "from sklearn.base import BaseEstimator\n",
                "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
                "from sklearn.metrics import log_loss\n",
                "from sentence_transformers import SentenceTransformer\n",
                "from lightgbm import LGBMClassifier\n",
                "from bertopic import BERTopic\n",
                "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance\n",
                "from bertopic.vectorizers import ClassTfidfTransformer\n\n",
                "# Configuration\n",
                "warnings.simplefilter(\"ignore\")\n",
                "sns.set_style(\"darkgrid\")\n",
                "pd.options.display.max_rows = None\n",
                "pd.options.display.max_columns = None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "  \"issue\": \"The analysis requires loading raw data from CSV files and performing initial cleaning, such as handling duplicates and parsing string-formatted lists.\",\n",
                "  \"action\": \"Define functions to load the training and test data using pandas. Implement a data cleaning function that removes the 'id' column from the training set, drops duplicate rows, and correctly parses the list-like string columns ('prompt', 'response_a', 'response_b') into actual Python lists, handling potential 'null' values.\",\n",
                "  \"state\": \"The data is loaded into pandas DataFrames, cleaned of duplicates, and all text-based columns are correctly formatted as lists, making them ready for detailed analysis.\"\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from pathlib import Path\n",
                "from typing import List, Tuple\n",
                "import pandas as pd\n\n",
                "def load_data(data_path: Path) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
                "    \"\"\"Loads train, test, and submission files.\"\"\"\n",
                "    train = pd.read_csv(data_path / \"train.csv\")\n",
                "    test = pd.read_csv(data_path / \"test.csv\")\n",
                "    sub = pd.read_csv(data_path / \"sample_submission.csv\")\n",
                "    return train, test, sub\n\n",
                "def clean_and_prepare_data(df: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"Cleans the dataframe by dropping unnecessary columns, duplicates, and parsing string lists.\"\"\"\n",
                "    if 'id' in df.columns:\n",
                "        df = df.drop(\"id\", axis=1)\n",
                "    \n",
                "    df = df.drop_duplicates(keep=\"first\", ignore_index=True)\n",
                "    \n",
                "    for col in [\"prompt\", \"response_a\", \"response_b\"]:\n",
                "        # Handle cases where the column might already be parsed or doesn't exist\n",
                "        if col in df.columns and isinstance(df[col].iloc[0], str):\n",
                "            try:\n",
                "                 # A more robust way to handle 'null' before eval\n",
                "                df[col] = df[col].apply(lambda x: eval(x.replace(\"null\", \"None\")))\n",
                "            except Exception as e:\n",
                "                print(f\"Could not parse column {col}. Error: {e}\")\n",
                "                # Fallback for columns that might not need parsing\n",
                "                df[col] = df[col].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
                "    return df\n\n",
                "DATA_PATH = Path(\"/kaggle/input/lmsys-chatbot-arena\")\n",
                "TARGETS = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
                "\n",
                "train_df, test_df, sub_df = load_data(DATA_PATH)\n",
                "train_df_cleaned = clean_and_prepare_data(train_df.copy())\n",
                "\n",
                "print(f\"Original train shape: {train_df.shape}\")\n",
                "print(f\"Cleaned train shape: {train_df_cleaned.shape}\")\n",
                "display(train_df_cleaned.head(2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "  \"issue\": \"To understand the competitive landscape of the models, we need to analyze their appearance frequency and head-to-head battle outcomes.\",\n",
                "  \"action\": \"Create functions to visualize the distribution of models appearing as 'model_a' and 'model_b' using pie charts. Then, develop a 'battle report' by pivoting the data to create heatmaps showing the number of battles and win rates between every pair of top models.\",\n",
                "  \"state\": \"Visualizations are generated that clearly show which models are most frequent and which models tend to win against others, providing a high-level overview of the model hierarchy.\"\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from typing import List, Optional\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n\n",
                "def plot_model_distribution(df: pd.DataFrame, thres: float = 0.02, max_labels: int = 5):\n",
                "    \"\"\"Plots the distribution of model_a and model_b identities.\"\"\"\n",
                "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 7))\n",
                "    model_counts = {\n",
                "        'model_a': df[\"model_a\"].value_counts(),\n",
                "        'model_b': df[\"model_b\"].value_counts()\n",
                "    }\n",
                "    \n",
                "    for ax, (model_col, counts) in zip(axes, model_counts.items()):\n",
                "        plot_pie_single(counts.values, counts.index.tolist(), f\"Distribution for {model_col}\", thres, max_labels, ax)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n\n",
                "def plot_pie_single(data: np.ndarray, labels: List[str], title: str, thres: float, max_labels: int, ax: plt.Axes):\n",
                "    \"\"\"Helper function to plot a single pie chart with minority aggregation.\"\"\"\n",
                "    tot = sum(data)\n",
                "    major_data = [(d, l) for d, l in zip(data, labels) if d / tot >= thres]\n",
                "    minor_data = [(d, l) for d, l in zip(data, labels) if d / tot < thres]\n",
                "    \n",
                "    if minor_data:\n",
                "        major_data.append((sum(d for d, _ in minor_data), \"Others\"))\n",
                "    \n",
                "    data, labels = map(list, zip(*major_data))\n",
                "    max_idx = np.argmax(data[:-1] if minor_data else data)\n",
                "    explode = [0.1 if i == max_idx else 0 for i in range(len(data))]\n",
                "    \n",
                "    patches, _ = ax.pie(data, startangle=140, colors=sns.color_palette(\"pastel\"), explode=explode)\n",
                "    ax.legend(patches, labels, title=\"Model Identity\", loc=\"center left\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
                "    ax.set_title(title, fontsize=16)\n\n",
                "def plot_battle_heatmaps(df: pd.DataFrame, top_n_models: int = 16):\n",
                "    \"\"\"Generates heatmaps for battle counts and win rates between top models.\"\"\"\n",
                "    top_models = set(df[\"model_a\"].value_counts().index[:top_n_models])\n",
                "    df_btl = df.groupby([\"model_a\", \"model_b\"], as_index=False).size().rename(columns={'size': 'battle_cnt'})\n",
                "    df_btl = df_btl.query(\"model_a in @top_models and model_b in @top_models\")\n",
                "\n",
                "    battle_pivot = df_btl.pivot(index=\"model_a\", columns=\"model_b\", values=\"battle_cnt\")\n",
                "    \n",
                "    fig, ax = plt.subplots(figsize=(12, 10))\n",
                "    sns.heatmap(battle_pivot, annot=True, fmt=\".0f\", cmap=\"rocket_r\", ax=ax)\n",
                "    ax.set_title(f\"Battle Count of Top-{top_n_models} Most Frequent Models\", fontsize=16)\n",
                "    plt.show()\n",
                "\n",
                "# --- Execution ---\n",
                "plot_model_distribution(train_df_cleaned)\n",
                "plot_battle_heatmaps(train_df_cleaned)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "  \"issue\": \"A key part of the analysis is understanding the verbosity of prompts and responses, as text length can be a strong predictor of user preference.\",\n",
                "  \"action\": \"Engineer features related to the length and number of turns in each conversation. Create functions to calculate the number of turns, the total character length of prompts and responses, and the difference in length between model A's and model B's responses. Then, visualize the distributions of these new features.\",\n",
                "  \"state\": \"The dataset is augmented with several new length-based features, and their distributions are plotted, revealing insights into conversational dynamics and model verbosity.\"\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def engineer_length_features(df: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"Engineers features based on the length of prompts and responses.\"\"\"\n",
                "    df['n_turns'] = df[\"prompt\"].apply(len)\n",
                "    \n",
                "    df['prompt_len'] = df['prompt'].apply(lambda x: sum(len(str(p)) for p in x))\n",
                "    df['response_a_len'] = df['response_a'].apply(lambda x: sum(len(str(r)) for r in x))\n",
                "    df['response_b_len'] = df['response_b'].apply(lambda x: sum(len(str(r)) for r in x))\n",
                "    \n",
                "    df['len_diff'] = df['response_a_len'] - df['response_b_len']\n",
                "    df['len_diff_abs'] = abs(df['len_diff'])\n",
                "    return df\n\n",
                "def plot_length_distributions(df: pd.DataFrame):\n",
                "    \"\"\"Visualizes the distributions of length-based features.\"\"\"\n",
                "    features_to_plot = ['n_turns', 'prompt_len', 'response_a_len', 'response_b_len', 'len_diff']\n",
                "    fig, axes = plt.subplots(len(features_to_plot), 1, figsize=(12, 15))\n",
                "    \n",
                "    for ax, feature in zip(axes, features_to_plot):\n",
                "        sns.histplot(df[feature], ax=ax, bins=50, kde=True)\n",
                "        ax.set_title(f'Distribution of {feature}', fontsize=14)\n",
                "        ax.set_xlabel('')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "# --- Execution ---\n",
                "train_df_lengths = engineer_length_features(train_df_cleaned.copy())\n",
                "plot_length_distributions(train_df_lengths)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "  \"issue\": \"To establish a performance baseline, we need to create and evaluate simple, rule-based models before moving to more complex machine learning approaches.\",\n",
                "  \"action\": \"Implement several naive baseline models: one that predicts a uniform 1/3 probability for all outcomes, one that predicts the global mean win/loss/tie rate, and a more refined version that predicts the mean rates specific to each model pair. Calculate the log loss for each baseline to measure its performance.\",\n",
                "  \"state\": \"Performance benchmarks are established from several naive models, providing a clear metric that more sophisticated models must surpass.\"\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from sklearn.metrics import log_loss\n\n",
                "def evaluate_uniform_baseline(df: pd.DataFrame) -> float:\n",
                "    \"\"\"Calculates log loss for a baseline that predicts 1/3 for all outcomes.\"\"\"\n",
                "    y_true = np.where(df[TARGETS].values)[1]\n",
                "    y_pred = np.ones((len(df), 3)) / 3\n",
                "    loss = log_loss(y_true, y_pred)\n",
                "    print(f\"Uniform Baseline Log Loss: {loss:.4f}\")\n",
                "    return loss\n\n",
                "def evaluate_mean_baseline(df: pd.DataFrame) -> float:\n",
                "    \"\"\"Calculates log loss for a baseline predicting the global mean.\"\"\"\n",
                "    y_true = np.where(df[TARGETS].values)[1]\n",
                "    mean_preds = df[TARGETS].mean().values\n",
                "    y_pred = np.tile(mean_preds, (len(df), 1))\n",
                "    loss = log_loss(y_true, y_pred)\n",
                "    print(f\"Global Mean Baseline Log Loss: {loss:.4f}\")\n",
                "    return loss\n\n",
                "# --- Execution ---\n",
                "evaluate_uniform_baseline(train_df_lengths)\n",
                "evaluate_mean_baseline(train_df_lengths)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "  \"issue\": \"To understand which engineered features are most predictive, we need to build a simple machine learning model and analyze its decision-making process.\",\n",
                "  \"action\": \"Train a Decision Tree Classifier using the length-based features created earlier. Use stratified K-fold cross-validation to get a robust estimate of its performance. Visualize the trained tree to interpret which features and thresholds are most important for predicting the winner.\",\n",
                "  \"state\": \"A baseline Decision Tree model is trained and evaluated, and its structure is visualized, providing initial insights into the predictive power of the engineered features.\"\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
                "from sklearn.metrics import log_loss\n\n",
                "def train_and_visualize_decision_tree(df: pd.DataFrame, features: List[str]):\n",
                "    \"\"\"Trains a Decision Tree and visualizes it.\"\"\"\n",
                "    X = df[features]\n",
                "    y = np.where(df[TARGETS].values)[1]\n",
                "\n",
                "    dt = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
                "    dt.fit(X, y)\n",
                "    \n",
                "    # Evaluate with cross-validation\n",
                "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
                "    losses = []\n",
                "    for train_idx, val_idx in skf.split(X, y):\n",
                "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
                "        y_train, y_val = y[train_idx], y[val_idx]\n",
                "        \n",
                "        model = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
                "        model.fit(X_train, y_train)\n",
                "        y_pred = model.predict_proba(X_val)\n",
                "        losses.append(log_loss(y_val, y_pred))\n",
                "    print(f\"Decision Tree CV Log Loss: {np.mean(losses):.4f} (+/- {np.std(losses):.4f})\")\n",
                "\n",
                "    # Visualize the tree\n",
                "    plt.figure(figsize=(20, 10))\n",
                "    plot_tree(dt, feature_names=features, class_names=TARGETS, filled=True, rounded=True, fontsize=10)\n",
                "    plt.title(\"Decision Tree Visualization\", fontsize=16)\n",
                "    plt.show()\n",
                "\n",
                "# --- Execution ---\n",
                "features = ['n_turns', 'prompt_len', 'response_a_len', 'response_b_len', 'len_diff', 'len_diff_abs']\n",
                "train_and_visualize_decision_tree(train_df_lengths, features)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```json\n",
                "{\n",
                "  \"issue\": \"To gain deeper insights from the raw text, we need to understand the main topics of discussion within the user prompts.\",\n",
                "  \"action\": \"Use BERTopic, a powerful topic modeling technique, to identify and visualize topics from the prompts. This involves generating sentence embeddings, using UMAP for dimensionality reduction and HDBSCAN for clustering, and then representing topics. The topics are then visualized to show their prevalence and relationships.\",\n",
                "  \"state\": \"A topic model is trained on the prompts, revealing the key themes and questions posed by users in the Chatbot Arena, which can be correlated with model performance.\"\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from sentence_transformers import SentenceTransformer\n",
                "from bertopic import BERTopic\n",
                "from umap import UMAP\n",
                "from hdbscan import HDBSCAN\n\n",
                "def perform_topic_modeling(prompts: List[str]):\n",
                "    \"\"\"Performs topic modeling on a list of prompts using BERTopic.\"\"\"\n",
                "    # Flatten the list of lists of prompts into a single list of strings\n",
                "    all_prompts = [p for sublist in prompts for p in sublist]\n",
                "\n",
                "    # For demonstration, we'll use a smaller subset\n",
                "    sample_prompts = all_prompts[:5000]\n",
                "\n",
                "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
                "    umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
                "    hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
                "    \n",
                "    topic_model = BERTopic(\n",
                "        embedding_model=embedding_model,\n",
                "        umap_model=umap_model,\n",
                "        hdbscan_model=hdbscan_model,\n",
                "        language=\"english\",\n",
                "        calculate_probabilities=True,\n",
                "        verbose=True\n",
                "    )\n",
                "    \n",
                "    topics, probs = topic_model.fit_transform(sample_prompts)\n",
                "    \n",
                "    print(\"--- Top Topics ---\")\n",
                "    display(topic_model.get_topic_info().head(10))\n",
                "    \n",
                "    # Visualize topics\n",
                "    try:\n",
                "        fig = topic_model.visualize_topics()\n",
                "        fig.show()\n",
                "    except Exception as e:\n",
                "        print(f\"Could not visualize topics. Error: {e}\")\n",
                "        \n",
                "    return topic_model\n",
                "\n",
                "# --- Execution ---\n",
                "prompts_list = train_df_lengths['prompt'].tolist()\n",
                "topic_model = perform_topic_modeling(prompts_list)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
