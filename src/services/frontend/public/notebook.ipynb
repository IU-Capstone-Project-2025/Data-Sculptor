{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "{\"title\": \"House Price Prediction\", \"difficulty_level\": \"Advanced\", \"rewards\": \"Comprehensive understanding of data preprocessing, feature engineering, model selection, and evaluation techniques\", \"task_statement\": \"A real estate company is facing a challenge in predicting the sale price of houses accurately. Due to the large number of features and the complexity of the data, manual analysis is not feasible. Your goal is to develop a machine learning model that can accurately predict the sale price of houses based on various features such as lot size, house style, and condition. You will need to preprocess the data, engineer new features, select the best model, and evaluate its performance. By successfully completing this task, you will contribute to the company's ability to make accurate predictions and improve their decision-making process. The company has hired you to automate this process using machine learning techniques.\", \"task_type\": \"Regression\", \"tools\": [\"numpy\", \"pandas\", \"matplotlib\", \"seaborn\", \"sklearn\", \"patsy\"]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Listing input data files\", \"We need to know what data is available to work with. Without this step, we might not know what files we have for analysis\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "029efcc8-dece-4be6-9443-80506447cdf7",
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "_uuid": "4745983da3c4d83522bfdacc0dad749d61ceed21"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Importing necessary libraries for data analysis and machine learning\", \"We need these libraries to perform data analysis, preprocessing, and model building. Without these libraries, we cannot perform the required tasks in data analysis and machine learning\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "43e42a9f-d705-420a-9b57-e6a1488d6d70",
        "_uuid": "861174e895dcc17cceff4d1c44658676cc1c05f7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#prep\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MaxAbsScaler, QuantileTransformer\n",
        "\n",
        "#models\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, LinearRegression, Ridge, RidgeCV\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "#validation libraries\n",
        "from sklearn.cross_validation import KFold, StratifiedKFold\n",
        "from IPython.display import display\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Import Data Analysis Libraries\",\n",
        "  \"issue\": \"Essential libraries for data analysis and machine learning are not imported in the Jupyter notebook, which is necessary for subsequent data analysis and model building tasks.\",\n",
        "  \"action\": \"Import necessary libraries for data analysis and machine learning using Python and Jupyter notebook. Libraries required include pandas for data processing, NumPy for linear algebra, matplotlib for data visualization, Sklearn for preprocessing and model building. Verify that all libraries have been imported by running the code.\",\n",
        "  \"state\": \"\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Loading the training and testing datasets\", \"We need the training and testing datasets to train our model and evaluate its performance. Without these datasets, we cannot proceed with the machine learning process\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "44fe3e05-0dee-4b04-bd9e-ecc1cbcbda05",
        "_uuid": "8e21cf3a1e6a4865182453f78bfa7668f6e800ab",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('../input/train.csv')\n",
        "df_test = pd.read_csv('../input/test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Creating a sample DataFrame from a dictionary\", \"We need to create a sample DataFrame to demonstrate or test a function or code. Without this, we cannot properly test or showcase how the function works\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Load Training and Testing Datasets\",\n",
        "  \"issue\": \"The machine learning model lacks the necessary datasets for training due to the absence of the training and testing data.\",\n",
        "  \"action\": \"Load the training and testing datasets from the specified file paths using pandas.read_csv and store them in pandas dataframe objects.\",\n",
        "  \"state\": \"All required libraries are imported and ready for data processing, analysis and machine learning model building. The imported libraries can be checked in the notebook's code cells.\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "86d132b4-3e2a-4034-b976-7d1d7e36f6eb",
        "_uuid": "0a543107eb22e3754e938caf980bb22f96acec9c",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# here's one sample\n",
        "sample_dict = [\n",
        "    {'label': 'house i would like', 'sqft':5000},\n",
        "    {'label': 'house i would hate','sqft':500},\n",
        "    {'label': 'house i live in', 'sqft':800}\n",
        "]\n",
        "pd.DataFrame(sample_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Checking the shape of the training dataset\", \"We need to know the dimensions of the training dataset to understand the size of the data we are working with. Without this information, it would be difficult to optimize the model and allocate the necessary resources for training\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Convert House Details to DataFrame\",\n",
        "  \"issue\": \"A list of dictionaries containing house details (label and square footage) is currently not in a format that facilitates data analysis and manipulation.\",\n",
        "  \"action\": \"Transform the list of dictionaries, each representing details about houses, into a pandas DataFrame, then display the DataFrame.\",\n",
        "  \"state\": \"Both datasets are successfully loaded into pandas dataframe objects and are ready for further processing and modeling.\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e237bd94-0e2a-43c2-8900-1986bb924245",
        "_uuid": "9c96a170cd21e1923a3732e0d0953153c2f2509e"
      },
      "outputs": [],
      "source": [
        "train_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Displaying statistical summary of training data\", \"To understand the basic statistics of the training data such as mean, standard deviation, min, max etc. Without this, we may miss important insights about the data distribution and scale\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "37e58ab9-f358-4df1-ad8a-95a4c522a937",
        "_uuid": "ee8f31fc2de1835d5733fa101f2991941ea0da39"
      },
      "outputs": [],
      "source": [
        "train_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Displaying the information about the training dataset\", \"We need to understand the structure of the training dataset, including the number of rows, columns, and the data types of each column. Without this information, it would be difficult to perform data analysis and preprocessing effectively\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ae23632e-4abf-49b4-a952-f07255fc08b9",
        "_uuid": "d9f07241b82fdd9b49a3cd69d25c9665c0943e4d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "train_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Displaying the first two rows of the training dataset\", \"To verify that the data has been loaded correctly and to get a quick glimpse of the data structure. Without this, we might proceed with incorrect data or assumptions about the data format\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Explore and Summarize Training Dataset\",\n",
        "  \"issue\": \"Lack of initial exploration and understanding of basic statistics, structural information, and data preview of the training dataset, which is required for effective data preprocessing, analysis, and validation.\",\n",
        "  \"action\": \"Display the statistical summary and information about the training dataset, followed by the first two rows of data, using pandas functions train_df.describe(), train_df.info(), and train_df.head(2) respectively, so that we can understand the basic data statistics, the structure of the dataset, and verify if the data has been loaded correctly.\",\n",
        "  \"state\": \"The list of dictionaries is converted into a pandas DataFrame, which is now ready for data analysis, manipulation, and visualization.\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7de83397-221c-44e4-bc0a-60140384c4d8",
        "_uuid": "cdd8b17a9df4f72b4cc86b88efdc8774bc91c71d"
      },
      "outputs": [],
      "source": [
        "train_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Displaying the first 5 values of the 'SalePrice' column in the training dataset\", \"To check the format and values of the 'SalePrice' column, which is crucial for understanding the data and proceeding with the analysis. Without this step, we might miss potential issues with the data, such as incorrect types or missing values\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6530f678-959f-4181-b875-fb274084d594",
        "_uuid": "b4bb574a0666967e4bded1e2962900a71634ce90"
      },
      "outputs": [],
      "source": [
        "train_df['SalePrice'].head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Displaying the first 5 rows of the 'SalePrice' column from the training dataset\", \"To check the format and values of the 'SalePrice' column in the training dataset, ensuring it is loaded correctly and ready for analysis. Without this check, we might proceed with incorrect or unexpected data, potentially leading to flawed analysis results.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dfad3eaa-0eb7-4a7f-84cf-3aaefcdcfd40",
        "_uuid": "a66a10627b06643aadacd0cea79850dda9aa77de"
      },
      "outputs": [],
      "source": [
        "train_df[['SalePrice']].head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Check 'SalePrice' Column Format\",\n",
        "  \"issue\": \"To check the format and values of the 'SalePrice' column in the training dataset, ensuring it is loaded correctly and ready for analysis. Without this check, we might proceed with incorrect or unexpected data, potentially leading to flawed analysis results.\",\n",
        "  \"action\": \"Display the first 5 rows of the 'SalePrice' column from the training dataset using the Jupyter notebook pandas DataFrame method .head(5)\",\n",
        "  \"state\": \"The statistical summary and structural information of the training dataset are displayed, as well as the first two rows of the data, providing a clear perspective of data scale, structure, and data format.\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Converting the training data frame to a matrix\", \"We need to convert the data frame to a matrix for further processing and analysis. Without this conversion, some machine learning algorithms may not be able to process the data properly\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c1c23f93-0854-43f5-836c-3cd1e6aa9ef6",
        "_uuid": "2e3848ffc7084afa5a2e1ff41b48a88a7a772bca",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "train_df.as_matrix()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Converting and displaying the first few rows of the training dataset as a DataFrame\", \"We need to convert the training dataset into a DataFrame and display the first few rows to understand the structure and format of the data. Without this, we cannot inspect the data and proceed with the analysis\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Convert Training DataFrame to Matrix\",\n",
        "  \"issue\": \"There is a need to prepare the training dataset for machine learning analysis. The training data is currently in the form of a pandas DataFrame, which when processed by certain machine learning models, might not work efficiently or may require additional setup. The data needs to be converted to a matrix that many machine learning algorithms can work with more easily.\",\n",
        "  \"action\": \"Convert the training data frame to a matrix using the 'DataFrame.as_matrix()' method for further data processing and analysis, and ensure that the data is compatible with machine learning algorithms.\",\n",
        "  \"state\": \"The first 5 values of the 'SalePrice' column are displayed, confirming the 'SalePrice' column is formatted correctly and contains the expected numerical values, ready for analysis.\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0984d429-421c-45c0-b600-2393b30d560f",
        "_uuid": "8ad8ea9a0349e5b04bce490fc73f067dfcbe47b6"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(train_df.as_matrix()).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Displaying the first 4 rows of the 'SalePrice' and 'LotShape' columns in the training dataset\", \"To check the data and understand the format of the 'SalePrice' and 'LotShape' columns. Without this, it would be difficult to proceed with data analysis and model building\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "229b56cd-a2d0-4876-9b51-bbabec28a2f9",
        "_uuid": "f88803e8c7858a4c226429cae671b2393a5b7be3"
      },
      "outputs": [],
      "source": [
        "train_df[['SalePrice','LotShape']].head(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Preview 'SalePrice' and 'LotShape' Columns\",\n",
        "  \"issue\": \"Without seeing the first few rows of 'SalePrice' and 'LotShape' columns, understanding the data types and formatting issues of these important features is hampered. This is crucial for running appropriate data analysis and predictive modeling algorithms.\",\n",
        "  \"action\": \"To fetch and display the top 4 rows of the 'SalePrice' and 'LotShape' columns from the training dataset using pandas dataframe indexing.\",\n",
        "  \"state\": \"The data is now in matrix form, which is compatible with machine learning models. The first few rows of the new matrix should be displayed to validate the conversion and ensure that data is ready for further analysis.\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Displaying the first three rows of the training dataset\", \"To check the format and structure of the training data. Without this, we may not be able to understand the data types and columns present in the dataset, which is crucial for further analysis and data manipulation.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ff6d8320-ba8a-46d4-ae22-419e6d0d759c",
        "_uuid": "a86efeaca62cb311acbf1c762f921826defb2056"
      },
      "outputs": [],
      "source": [
        "train_df.iloc[range(3),]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Displaying the top 3 rows of the train dataset with SalePrice greater than 200000\", \"The purpose of this code is to filter and visualize the top rows of the training dataset where the SalePrice is above 200000. This helps in understanding the characteristics of high-value sales. Without this, we might overlook important patterns in the high-value data\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Analyze High-Value Sales\",\n",
        "  \"issue\": \"To analyze the characteristics of high-value sales in the training dataset, it is crucial that we can filter and read a sample of sales with prices greater than 200000. Without this step, we could miss out on understanding significant patterns in high-value properties.\",\n",
        "  \"action\": \"Filter and display the top 3 rows of the training dataset where SalePrice is greater than 200000 using train_df[train_df['SalePrice'] > 200000].iloc[range(3),:]. This process enables us to inspect the features associated with high-value sales. Use pandas for data manipulation tasks.\",\n",
        "  \"state\": \"The display of top 4 rows of the selected columns is achieved, which aids in preliminary data inspection. Checking the displayed output for data types, any missing or NaN values for 'SalePrice' and 'LotShape' will ensure the readiness of the dataset for further processing.\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "468147d7-9ade-4788-996e-2c0856b9cc89",
        "_uuid": "79f444576581497ce5c6cdb8bbc4abee548d9747"
      },
      "outputs": [],
      "source": [
        "train_df[train_df['SalePrice']>200000].head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Filtering and displaying the first three rows of the training dataset where 'LotShape' is either 'Reg' or 'IR1'\", \"The purpose of this code is to filter and display specific rows from the training dataset based on the 'LotShape' column. This is useful for data analysis and understanding the characteristics of the dataset. Without this code, we would not be able to inspect the dataset based on these specific conditions.\".\n",
        "\n",
        "\"Filtering and displaying the first three rows of the training dataset where 'LotShape' is either 'Reg' or 'IR1'\", \"The purpose of this code is to inspect the data where the lot shape is regular or irregular type 1, which might be important for understanding the distribution or patterns in these specific lot shapes\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cf902d73-d3a0-4cbb-9a71-02b5b6ed1609",
        "_uuid": "6fc548d131d761f312d00c8f09555c17c2b10bfc"
      },
      "outputs": [],
      "source": [
        "train_df[train_df['LotShape'].isin(['Reg','IR1'])].head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Analyze Lot Shape Distribution\",\n",
        "  \"issue\": \"The dataset's characteristics in terms of lot shape, which can be regular ('Reg') or irregular ('IR1'), require examination for data analysis purposes. Without this step, insights into patterns or distributions specific to these lot shapes cannot be obtained.\",\n",
        "  \"action\": \"Filter the training dataset 'train_df' based on the conditions where the 'LotShape' column values are either 'Reg' (regular) or 'IR1' (irregular type 1), displaying the first three rows that match these conditions.\",\n",
        "  \"state\": \"The filtered portion of the dataset with high-value sales is successfully displayed, allowing for a detailed examination of the properties and features associated with these sales. Data completeness and integrity can be checked.\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Printing the number and names of columns in the training dataset\", \"We need to verify that the data has been loaded correctly and to understand the structure of the data. Without this, we may not be aware of the data's layout and could make incorrect assumptions during analysis\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d404bc01-c982-4357-9c63-7d97dbf5d2d6",
        "_uuid": "f6119c8bc715b7232e6b900d8f0c2f3813b95ca3"
      },
      "outputs": [],
      "source": [
        "print('this many columns:%d ' % len(train_df.columns))\n",
        "train_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Renaming columns in the training dataset\", \"The purpose of this code is to rename the columns in the training dataset for easier reference and manipulation. Without this, the column names would be difficult to work with and could lead to errors in the analysis.\".\n",
        "\n",
        "Great, now generate the name of action and its purpose for the following code snippet:\n",
        "{\"cell_type\":\"code\",\"source\":[\"# Importing necessary libraries\\n\",\"import pandas as pd\\n\",\"import numpy as np\\n\",\"import matplotlib.pyplot as plt\\n\",\"import seaborn as sns\\n\",\"\\n\",\"# Reading the dataset\\n\",\"df = pd.read_csv('data.csv')\\n\",\"\\n\",\"# Displaying the first 5 rows of the dataset\\n\",\"df.head()\"],\"metadata\":{\"_cell_guid\":\"a5fe1fca-5535-41c1-8d13-02a65791eb64\",\"_uuid\":\"0cfa1eacf242e7fb321a44ad30860457b50c30ba\",\"collapsed\":false}} \"Importing libraries and loading the dataset\", \"The purpose of this code is to import necessary libraries for data manipulation and visualization, load the"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Load and Prepare Dataset\",\n",
        "  \"issue\": \"The project requires data from a CSV file to conduct analysis and visualization, but the necessary libraries and data are not yet loaded.\",\n",
        "  \"action\": \"Import necessary Python libraries and load the dataset from a CSV file.\",\n",
        "  \"state\": \"The code fragment has successfully filtered and displayed the first three rows of the 'train_df' dataset where the lot shape is either 'Reg' or 'IR1'. The inspected rows are now available for data analysis, with the lot shape conditions met.\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a5fe1fca-5535-41c1-8d13-02a65791eb64",
        "_uuid": "0cfa1eacf242e7fb321a44ad30860457b50c30ba",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "train_df.columns = ['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
        "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
        "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
        "       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
        "       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
        "       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
        "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n",
        "       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
        "       'HeatingQC', 'CentralAir', 'Electrical', 'FirsstFlrSF', 'SecondFlrSF',\n",
        "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
        "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
        "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
        "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
        "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
        "       'EnclosedPorch', 'ThreeSsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
        "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
        "       'SaleCondition', 'SalePrice']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Displaying the first 10 rows of the training dataset where 'Alley' column is null\", \"The purpose of the code is to identify and visualize missing data in the 'Alley' column of the training dataset. This is crucial for data cleaning and preprocessing, as missing data can affect the accuracy of the analysis. If we do not address missing data, it can lead to biased or incorrect results\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ad903f67-116d-4cd6-b238-cc241d2a9479",
        "_uuid": "bc49a567ddc5d3b0e64a432f17a8113e89a4f859"
      },
      "outputs": [],
      "source": [
        "train_df[train_df['Alley'].isnull()].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Filling missing values in 'Alley' column with 0\", \"We need to handle missing values in the dataset to avoid errors during model training. If we don't do this, the model may not be able to process the data correctly, leading to inaccurate predictions.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Handle Missing Values in 'Alley' Column\",\n",
        "  \"issue\": \"In order to ensure accurate model training and predictions, all null or missing values in the dataset must be addressed. Ignoring or leaving missing data unprocessed in the 'Alley' column could lead to biased or incomplete model outputs, impacting the reliability and effectiveness of the analysis. For data quality and model integrity, missing values should be filled or handled appropriately.\",\n",
        "  \"action\": \"The code identifies rows in the training dataset where the 'Alley' column contains null values and fills them with 0. This is achieved by selecting rows with 'Alley' as null using the isnull() method, displaying the first 10 of such rows, and subsequently replacing these missing values with 0, which can be interpreted as an absence of a specific feature in the context of the dataset. The pandas DataFrame .head(10) method is used to display the first 10 rows of null values in the 'Alley' column, and the missing values are filled with 0 to ensure that the dataset is complete and does not introduce bias into the analysis.\",\n",
        "  \"state\": \"The project now has access to the dataset and the necessary libraries for data manipulation and analysis.\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2780bbb7-c7d0-4db0-8726-b325dc981ea9",
        "_uuid": "4d161a32b30813e74c1874a7b3f7f3ad91ca5b2f",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "train_df['Alley'].fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Calculating the total number of missing values in the training dataset\", \"We need to understand the extent of missing data in our training dataset to handle it appropriately. If we do not check for missing values, it can lead to incorrect analysis or model predictions\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Fill Missing Values in 'Alley' Column\",\n",
        "  \"issue\": \"There are missing values in the 'Alley' column of the training dataset, which can lead to incorrect analysis and model predictions.\",\n",
        "  \"action\": \"Fill in missing values in the training dataset's 'Alley' column with '0' using the pandas DataFrame method df.fillna.\",\n",
        "  \"state\": \"After executing the action, null values in the 'Alley' column will be replaced with a value of 0, signifying the absence of a specific feature, which will be clearly identifiable and will not introduce bias into the analysis or model predictions. This dataset, now complete with no missing values, will allow for accurate and unbiased model training and predictions, and the dataset will be ready for analysis and modeling.\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b324a6a7-03b2-4554-95cf-066196c05c5a",
        "_uuid": "351d74540ead368d528116e453d4037063bd9279"
      },
      "outputs": [],
      "source": [
        "na_totals = train_df.isnull().sum().sort_values(ascending=False)\n",
        "na_totals[na_totals>0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Identify Missing Values in Training Dataset\",\n",
        "  \"issue\": \"There is a potential issue with columns in the training dataframe(train_df) that may contain missing values which need to be identified and possibly addressed in data preprocessing for better modeling.\",\n",
        "  \"action\": \"Run the provided code cell to calculate the total number of missing values in each column of the training dataset. The total number of missing values is sorted in descending order to identify the columns that have missing values.\",\n",
        "  \"state\": \"The 'Alley' column in the training dataset has been populated with '0' in place of missing values. This makes the dataset complete and ready for further analysis and model building, providing a verified state of data integrity.\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Load Black Friday Sales Dataset\",\n",
        "  \"issue\": \"The Black Friday Sales dataset required for data analysis and building the machine learning model is not being loaded.\",\n",
        "  \"action\": \"Load the Black Friday Sales dataset from the specified file path using pandas.read_csv library and store the loaded data in a pandas dataframe object.\",\n",
        "  \"state\": \"After running the code, columns with missing values and their counts are displayed, allowing for the data scientist to decide on appropriate actions, such as data imputation or removal of these columns based on their significance.\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Filling missing values in the training dataset with zeros\", \"This is necessary to ensure that the data is clean and ready for analysis. If missing values are not handled, it can lead to incorrect results or errors during model training\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "03bc1f54-9b5b-43aa-9a49-9f4c50adf150",
        "_uuid": "0cb87f7ba4fffe32dfacc963323e73ec0d66df04",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "train_df.fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Converting selected columns to float type\", \"We need to ensure that the selected columns are in the correct data type for further analysis. If we do not do that, we may encounter errors during data processing or analysis\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "eefb9dac-6412-43f5-b922-50c7740eef33",
        "_uuid": "4fb6e09c3625896598959e291e5e2b62923a52ed"
      },
      "outputs": [],
      "source": [
        "numeric_cols = [x for x in train_df.columns if ('Area' in x) | ('SF' in x)] + ['SalePrice','LotFrontage','MiscVal','EnclosedPorch','ThreeSsnPorch','ScreenPorch','OverallQual','OverallCond','YearBuilt']\n",
        "\n",
        "for col in numeric_cols:\n",
        "    train_df[col] = train_df[col].astype(float)\n",
        "numeric_cols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Converting categorical columns to category data type\", \"We need to convert categorical columns to category data type for better memory usage and faster processing. If we do not do that, the categorical data will be treated as object data type which can lead to slower processing and higher memory usage\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "93323fb4-21ee-4239-ae93-5d026135d16e",
        "_uuid": "da896e2385e79b3afc1476dedb628d2658bda10a",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "categorical_cols = [x for x in train_df.columns if x not in numeric_cols]\n",
        "\n",
        "for col in categorical_cols:\n",
        "    train_df[col] = train_df[col].astype('category')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Handle Missing Values and Data Types in Training Dataset\",\n",
        "  \"issue\": \"The training dataset contains missing values and incorrect data types for certain columns. This can lead to unreliable data analysis results and affect the performance of machine learning models due to an increase in memory usage and slower processing speeds.\",\n",
        "  \"action\": \"Fill missing values in the training dataset with zeros. This is necessary to ensure that the data is clean and ready for analysis. If missing values are not handled, it can lead to incorrect results or errors during model training. Furthermore, any missing categorical data is treated by converting it to a category data type to facilitate memory savings and faster processing, while converting selected columns to float type to preclude any errors during data processing/analysis and any possible categorical data is treated by converting it to a category data type to facilitate memory savings and faster processing.\",\n",
        "  \"state\": \"Dataset is loaded into a pandas dataframe and ready for analysis and modeling.\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Applying a logarithmic transformation to the 'SalePrice' column\", \"The purpose of this code is to apply a logarithmic transformation to the 'SalePrice' column in the training dataset. This is often done to normalize the distribution of the data, which can improve the performance of certain machine learning algorithms. Without this transformation, the model may not perform as well due to the skewed distribution of the 'SalePrice' data.\" \n",
        "\n",
        "However, as per your instruction, here is the simplified version: \n",
        "\"Applying a logarithmic transformation to the 'SalePrice' column\", \"This is done to normalize the data distribution for better model performance.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "591c22ae-1ace-47bc-bd65-b6f1eec22352",
        "_uuid": "b2b3133ac2149ae0d96cf62c5f23c7e8d80c0a33",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "## Applying an element wise function\n",
        "train_df['LogSalePrice'] = train_df['SalePrice'].map(lambda x : np.log(x)) \n",
        "\n",
        "#Vectorized log function acting on a vector\n",
        "# then assigning all the values at once\n",
        "train_df['LogSalePrice'] = np.log(train_df['SalePrice'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Plotting the histogram of SalePrice\", \"We need to visualize the distribution of SalePrice to understand its characteristics and identify any potential issues with the data, such as outliers or skewness. Without this step, we may not be able to accurately model the relationship between SalePrice and other variables\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cd37ff57-ff62-4039-9699-c7ec019e7b4a",
        "_uuid": "7e6ad0b16f665b1e2593fe0249bc56f1ebf91434"
      },
      "outputs": [],
      "source": [
        "train_df['SalePrice'].hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Plotting the histogram of LogSalePrice\", \"We need to visualize the distribution of the LogSalePrice to understand its characteristics and identify any potential issues with the data, such as skewness or outliers. Without this step, we may not be able to accurately model the relationship between the features and the sale price\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Normalize 'SalePrice' Distribution\",\n",
        "  \"issue\": \"The skewed distribution of 'SalePrice' data in the training dataset requires normalization for better machine learning model accuracy.\",\n",
        "  \"action\": \"The logarithmic transformation is to be applied to the 'SalePrice' column in the training dataset using numpy's log function to normalize the distribution of sales prices for improved machine learning model performance\",\n",
        "  \"state\": \"Missing values are handled by replacing them with zeros, numerical and categorical columns are correctly set to float and category data types respectively. The dataset is now clean and optimized for data analysis/processing and any possible categorical data is treated by converting it to a category data type to facilitate memory savings and faster processing.\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "13d70bac-b430-4248-97b1-627576d8e86e",
        "_uuid": "b9877b7b4b4230b06b7cef7a3ad5c09e1661b6a9"
      },
      "outputs": [],
      "source": [
        "train_df['LogSalePrice'].hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Creating a new column 'above_200k' in the training dataset\", \"We need to categorize the SalePrice into two groups for better analysis and understanding of the data. If we do not do that, we might miss out on some important insights that can be obtained by grouping the SalePrice\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f14e19c4-d9df-40cd-9702-386e5394b922",
        "_uuid": "2907421cee60cee8c58c2776602828db3fefc42b",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# element wise function to transform\n",
        "train_df['above_200k'] = train_df['SalePrice'].map(lambda x : 1 if x > 200000 else 0) \n",
        "train_df['above_200k'] = train_df['above_200k'].astype('category')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Creating a new categorical column 'above_200k' based on 'SalePrice'\", \"The purpose of this code is to categorize the 'SalePrice' into two categories, above 200k and below or equal to 200k, which can be used for further analysis or model training. Without this, the data would not be segmented based on price, potentially missing out on price-based patterns or insights.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0f456c78-1417-43d6-b4a0-74ba02890eb5",
        "_uuid": "8e2d7f47e5536a414d0a5c3056a3b542d6515e8f",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# manually assign the values to your new field, section by section\n",
        "# with row filtering\n",
        "train_df.loc[train_df['SalePrice']>200000,'above_200k'] = 1\n",
        "train_df.loc[train_df['SalePrice']<=200000,'above_200k'] = 0\n",
        "train_df['above_200k'] = train_df['above_200k'].astype('category')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Creating a new feature 'LivArea_Total' by combining 'GrLivArea', 'GarageArea', and 'PoolArea'\", \"We need to create new features to enrich the dataset and potentially improve the predictive power of our model. Without this, we might be missing out on important information that could affect the outcome of our analysis\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0d649fa3-375f-472e-bda5-45ba5ce56e4f",
        "_uuid": "34fea4f0f84dec44b1178d1a925b2c3a32fa987f"
      },
      "outputs": [],
      "source": [
        "train_df['LivArea_Total'] = train_df['GrLivArea'] + train_df['GarageArea'] + train_df['PoolArea']\n",
        "train_df[['LivArea_Total','GrLivArea','GarageArea','PoolArea']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Concatenating MSZoning and LotShape columns in train_df to create a new column Lot_desc\", \"The purpose of this code is to combine two categorical variables into a single variable for further analysis. This can help in reducing the dimensionality of the data and potentially create a more meaningful feature. Without this step, we would have to analyze the two variables separately, which might not reveal the full relationship between the variables and the target\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "419c2a12-4bec-498a-86d1-b70ad670694b",
        "_uuid": "682e3789c6b8e719c11d9bb84997f2ff71c8a2c1"
      },
      "outputs": [],
      "source": [
        "## concatenating two different fields together in the same row\n",
        "train_df['Lot_desc'] = train_df.apply(lambda val : val['MSZoning'] + val['LotShape'], axis=1)\n",
        "train_df[['Lot_desc','MSZoning','LotShape']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Calculate Total Liveable Area\",\n",
        "  \"issue\": \"The dataset does not have a calculated total for liveable area, making it difficult to assess the total area available for living in a house.\",\n",
        "  \"action\": \"Creating a new feature 'LivArea_Total' by combining 'GrLivArea', 'GarageArea', and 'PoolArea', and calculating the total liveable area.\",\n",
        "  \"state\": \"The 'SalePrice' column is normalized through a logarithmic transformation, creating a 'LogSalePrice' column with a more balanced data distribution, ready for model training and analysis.\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Normalizing and Scaling the LotArea feature\", \"We need to normalize and scale the LotArea feature to ensure that our model is not biased towards this feature due to its scale. Without normalization and scaling, features with larger values can dominate the learning process, leading to a less accurate model.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "43c2c24a-0041-4d6c-94ed-a34c2d960b58",
        "_uuid": "f61856304dc2f5982201c7c7ef1c8dbf2c82441e"
      },
      "outputs": [],
      "source": [
        "train_df['LotArea_norm'] = train_df['LotArea']\n",
        "\n",
        "ss = StandardScaler()\n",
        "mas = MaxAbsScaler()\n",
        "qs = QuantileTransformer()\n",
        "\n",
        "train_df['LotArea_norm'] = ss.fit_transform(train_df[['LotArea']])\n",
        "train_df['LotArea_mas'] = mas.fit_transform(train_df[['LotArea']])\n",
        "train_df['LotArea_qs'] = qs.fit_transform(train_df[['LotArea']])\n",
        "\n",
        "\n",
        "train_df[['LotArea_norm','LotArea_mas','LotArea_qs', 'LotArea']].head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Creating a smaller dataframe with 'MSZoning' and 'SalePrice' columns\", \"We need a smaller dataframe to analyze the relationship between 'MSZoning' and 'SalePrice'. Without this, it would be difficult to understand the impact of 'MSZoning' on 'SalePrice'\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d613ebc5-739e-4ce0-9455-22f1e5f6cc4f",
        "_uuid": "6db1f4962ad52fe17dfe3df0ece5e3a733bedcfb"
      },
      "outputs": [],
      "source": [
        "small_df = train_df[['MSZoning','SalePrice']].copy()\n",
        "small_df['MSZoning'] = small_df['MSZoning'].astype('category')\n",
        "small_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Scale 'LotArea' Feature\",\n",
        "  \"issue\": \"The 'LotArea' feature may dominate the learning process due to its scale, potentially leading to a biased model.\",\n",
        "  \"action\": \"Normalize and scale the 'LotArea' feature using StandardScaler, MaxAbsScaler, and QuantileTransformer methods.\",\n",
        "  \"state\": \"New feature 'LivArea_Total' has been added to the dataframe, reflecting the total liveable area, and is ready for inclusion in the analysis and modeling processes.\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Creating dummy variables for categorical data\", \"We need to convert categorical data into a format that can be provided to ML algorithms to improve model accuracy. If we do not do that, the algorithms may interpret the categories as numerical values, leading to incorrect analysis\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "28a662ee-b4ad-4049-b2f0-152083d5fcf8",
        "_uuid": "a11fdd30c4d762863e872b6c5c7469c7e2af5495"
      },
      "outputs": [],
      "source": [
        "pd.get_dummies(small_df).head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Convert Categorical Data to Dummy Variables\",\n",
        "  \"issue\": \"Categorical data may be incorrectly interpreted as numerical by machine learning algorithms, risking incorrect model predictions and decreased accuracy.\",\n",
        "  \"action\": \"Convert the categorical columns of the 'small_df' DataFrame by generating dummy variables using pandas' get_dummies function.\",\n",
        "  \"state\": \"The 'LotArea' feature is now normalized and scaled using three different methods (Standardization, Max Absolute Scaling, Quantile Transformation), ensuring that the model is not biased towards this feature due to its scale.\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Creating a smaller dataframe with 'MSSubClass' and 'SalePrice' columns and converting 'MSSubClass' to category type\", \"We need this code to create a smaller, more manageable dataset for analysis and to ensure that the 'MSSubClass' column is treated as categorical data, which is important for certain types of analysis and modeling. Without this code, we would not be able to perform specific analyses or use certain machine learning algorithms that require categorical data to be properly encoded\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bbab9c27-1408-4faa-b3d8-5bbd43482653",
        "_uuid": "43d56c57f4586bda6f0937c8f7f67b6d940da0d0"
      },
      "outputs": [],
      "source": [
        "small_df = train_df[['MSSubClass','SalePrice']].copy()\n",
        "small_df['MSSubClass'] = small_df['MSSubClass'].astype('category')\n",
        "small_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Encoding the 'MSSubClass' column\", \"We need to convert categorical data into numerical data for machine learning models to process. Without this step, the model cannot interpret the categorical data and the analysis cannot be completed.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "72a59da6-4888-43da-838c-7ed03e06572f",
        "_uuid": "84a4f8c27327c991bc7453335645819f4d736005"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "trf_MSSubClass = le.fit_transform(small_df['MSSubClass'])\n",
        "trf_MSSubClass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Getting the classes of the LabelEncoder\", \"We need to know the classes that were used in the LabelEncoder to properly decode the encoded labels. Without this information, we cannot convert the encoded labels back to their original form\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "08a8f323-a4bd-4d26-97cd-c6ea03632840",
        "_uuid": "feb3235294471ea6578d8c97710e4beaf0befe5e"
      },
      "outputs": [],
      "source": [
        "le.classes_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Converting numerical values back to categorical values for MSSubClass\", \"The purpose of this code is to reverse the encoding of the MSSubClass feature, which was previously transformed into numerical values, back to its original categorical values. This is necessary for proper interpretation of the results and for any further analysis that requires the categorical information of MSSubClass\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Create Smaller Dataset with Categorical 'MSSubClass'\",\n",
        "  \"issue\": \"A specific, smaller dataset with the 'MSSubClass' and 'SalePrice' columns, with 'MSSubClass' converted to a categorical type, is required for more detailed analysis and certain model building processes. Without this code, the 'MSSubClass' column will be treated as an integer, which may lead to inappropriate assumptions in the data analysis.\",\n",
        "  \"action\": \"Create a smaller dataframe named 'small_df' from the 'train_df' pandas dataframe by selecting only the 'MSSubClass' and 'SalePrice' columns. Then, convert the 'MSSubClass' column to a category data type.\",\n",
        "  \"state\": \"Dummy variables for categorical features have been created. These can now be used as input for machine learning algorithms without risking incorrect interpretations.\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2f1c6226-e9c6-4ade-b5e1-673779649ae8",
        "_uuid": "74363dce78029dc5eb90cd99edc9bcf3b4f50124"
      },
      "outputs": [],
      "source": [
        "le.inverse_transform(trf_MSSubClass)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Creating a list of feature columns\", \"We need to separate the features from the target variable to train our model. Without this step, we cannot proceed with the model training process\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "47070e8c-6221-4fee-aedb-927f33b6172c",
        "_uuid": "87b94a16bbbe37aac1b0a2144acb5165329273ba",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "feature_cols = [col for col in train_df.columns if 'Price' not in col]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Extracting target variable and features from the training dataset\", \"We need to separate the target variable (LogSalePrice) and the features (X) from the training dataset to train our model. Without this step, we cannot proceed with the model training process\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "abc80dfb-dca6-4cd5-80ba-ff27595f0bf9",
        "_uuid": "5a4f21ffb9ae866df4a7c9c29ebabf0a1ab61aca"
      },
      "outputs": [],
      "source": [
        "y = train_df['LogSalePrice']\n",
        "X = train_df[feature_cols]\n",
        "print(y.head(2),'\\n\\n', X.head(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Separate Features and Target\",\n",
        "  \"issue\": \"Need to separate the features and the target variable in order to train the machine learning model. Without this separation, the model cannot be trained.\",\n",
        "  \"action\": \"Separate the feature columns from the target variable column ('LogSalePrice') in the training dataset, create a list of feature columns that does not include the target variable. Extract features (X) and target variable (y) from the training dataset.\",\n",
        "  \"state\": \"Features and target variable have been successfully separated.\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Converting categorical data to numerical data\", \"We need to convert categorical data to numerical data because machine learning algorithms can only process numerical data. If we do not do that, we will not be able to train our model\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "570c5776-89e0-42e4-bfe5-d7b6e99ed830",
        "_uuid": "a8b1fc00333944b2cd067ccf9c20e8fc2f1ccf23"
      },
      "outputs": [],
      "source": [
        "X_numerical = pd.get_dummies(X)\n",
        "X_numerical.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Generating Design Matrices for Model Training\", \"We need to convert our data into a format that can be used by our statistical model. Without this step, the model cannot be trained and predictions cannot be made\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "805eda88-ba3e-4b4e-9fce-05c52d17a00a",
        "_uuid": "7166819d5ae904589e192ab0588a6ababf0e4145"
      },
      "outputs": [],
      "source": [
        "import patsy\n",
        "formula = 'LogSalePrice ~ %s' % (' + '.join(feature_cols)) \n",
        "y, X = patsy.dmatrices(formula, train_df, return_type='dataframe')\n",
        "print(y.head(2),'\\n\\n', X.head(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Splitting the dataset into training and validation sets\", \"We need to split the data into training and validation sets to evaluate the model's performance and prevent overfitting. Without this step, we cannot accurately assess the model's ability to generalize to new data\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e5b5747d-9825-450b-b9f5-5d4fb3c4f9df",
        "_uuid": "848805b5af5f045bfbe8d4f33fe863eb55d7b1ee"
      },
      "outputs": [],
      "source": [
        "def split_vals(a,n): return a[:n], a[n:]\n",
        "n_valid = 170\n",
        "n_trn = len(y)-n_valid\n",
        "X_train, X_valid = split_vals(X, n_trn)\n",
        "y_train, y_valid = split_vals(y, n_trn)\n",
        "\n",
        "print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Splitting the data into training and validation sets\", \"We need to evaluate the model's performance on unseen data to ensure it generalizes well. Without this, we risk overfitting to the training data and getting false confidence in our model's performance\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "60557fda-5da4-4834-a746-18a75ba94f15",
        "_uuid": "d4151eb948d338721db8df19ae60fa7d21247733"
      },
      "outputs": [],
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.2)\n",
        "print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Convert Categorical to Numerical\",\n",
        "  \"issue\": \"Categorical data in the dataset is preventing the machine learning model from being trained and making predictions.\",\n",
        "  \"action\": \"Convert categorical data to numerical data using the pd.get_dummies function.\",\n",
        "  \"state\": \"Categorical data has been successfully converted to numerical data.\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Fitting a Linear Regression model\", \"We need to train the model on the training data to be able to make predictions on the test data. If we do not do that, we will not have a model to make predictions and complete the analysis\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e5f1e7ee-a82b-4617-a2ed-519bded97bd3",
        "_uuid": "7465323a658bef722d904c909df374df5f15008d"
      },
      "outputs": [],
      "source": [
        "lm = LinearRegression()\n",
        "lm.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Evaluating the linear model's performance on the training data\", \"We need to evaluate the performance of the model on the training data to understand how well it is learning the patterns in the data. Without this step, we would not know if the model is overfitting or underfitting the data\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d2e3a53a-e570-4bb6-99b6-5fd9c7d8284e",
        "_uuid": "38bb4e88c86ca9372f13c5073e39bd810afa31d7"
      },
      "outputs": [],
      "source": [
        "lm.score(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Train Linear Regression Model\",\n",
        "  \"issue\": \"The Linear Regression model needs to be trained on the training dataset to make predictions.\",\n",
        "  \"action\": \"Fit linear regression model on training dataset using Scikit-Learn's LinearRegression().\",\n",
        "  \"state\": \"The Linear Regression model has been successfully trained on the training dataset.\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Evaluating the model's performance on the validation set\", \"We need to evaluate the model's performance on the validation set to understand how well the model is generalizing to unseen data. If we do not do this, we may not be able to accurately assess the model's performance and may overfit or underfit the model to the training data.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cfab3376-cfc7-491f-8d20-93d0752c1134",
        "_uuid": "5f9969114c685dcaf62f1870b5b8244d5032aec1"
      },
      "outputs": [],
      "source": [
        "lm.score(X_valid,y_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Validate Linear Regression Model\",\n",
        "  \"issue\":\"The evaluation of the simple linear regression model 'lm' on the training set can lead to an inaccurate understanding of the model's performance for unseen data - leading to either overfitting or underfitting. Without evaluating the model on a separate validation dataset, i.e., 'X_valid' and 'y_valid', we will not be able to precisely understand the model's ability to generalize and the reliability of our predictions on unseen data. This can have crucial implications for the integrity of the model and reliability of the predictions.\",\n",
        "  \"action\":\"To evaluate the model's performance on the validation set, we will utilize the 'score' method from the linear model from the 'scikit-learn' library. This method calculates the coefficient of determination R^2, indicating the percentage of the desired response (Y) variance that can be explained using the simple linear regression model applied to the desired dataset (X). In this context, we specifically use 'X_valid' and 'y_valid' subsets from the validation dataset to validate the 'lm' model. Once applied, this will evaluate the fitment of the model from the 'lm' model on the 'X_valid' dataset (independent validation data), from which 'y_valid' (dependent validation data) can be predicted.\",\n",
        "  \"state\":\"The Linear Regression model is successfully trained and evaluated on the training data with performance metrics. The model is ready for predictions and further analysis.\"\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Evaluating the model's performance\", \"We need to assess how well the model predicts the target variable. Without this step, we cannot determine if the model is performing well or not.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e2ade1af-d814-4c88-80b7-8c52f2eb19c8",
        "_uuid": "071754d583c2a1f1789afc582da8d588b88f0b35"
      },
      "outputs": [],
      "source": [
        "y_pred = lm.predict(X_valid)\n",
        "rmse = np.sqrt(metrics.mean_squared_error(y_pred, y_valid))\n",
        "rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Fitting RidgeCV model with different alphas\", \"We need to find the optimal alpha value for the RidgeCV model to avoid overfitting and underfitting. Without this, we cannot ensure the best performance of the model\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\"title\": \"Evaluate and Optimize Model\",\"issue\":\"The predictiveness of the model after training is unknown. With no performance evaluation, there's no guarantee that the model works well. Also, we have no assurance that the RidgeCV model uses the best alpha value.\",\"action\":\"Predict the target variable using the trained linear model on validation data, then calculate the root mean squared error (RMSE) to assess the model's predictive power. Also fit the RidgeCV model with various alpha values to find the optimal regularization parameter.\",\"state\":\"Upon evaluating the model using the 'score' method and applying it to the validation data, a R^2 score reflecting the model's fit to the desired dataset (X_valid) will be obtained. This metric is a measure of the variance of y_valid that can be predicted by the model, allowing for an accurate evaluation of the model's performance and generalizability. This R^2 score will be inspected for confirmation of successful model evaluation on the validation data, ensuring the integrity of the model and reliability of predictions.\"}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cde90b63-2833-459f-be59-9acac2e39b40",
        "_uuid": "08ba32853846fde477af16f081134c1b6f34a664"
      },
      "outputs": [],
      "source": [
        "rdgCV = RidgeCV(alphas=[0.01,0.1,1,10,100,1000], cv=5)\n",
        "rdgCV.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Printing the alpha value of the RidgeCV model\", \"We need to know the optimal alpha value for the RidgeCV model to ensure that it is properly regularized. Without this step, we may not be able to accurately predict the target variable\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8e9995c9-522a-452c-9dc8-14cd489275c6",
        "_uuid": "1db9458ca2e628fa1d627156ee00c23095d0c914"
      },
      "outputs": [],
      "source": [
        "print(rdgCV.alpha_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\"title\": \"Optimize RidgeCV Model\",\"issue\":\"We don't know the optimal alpha value for the RidgeCV model, which is essential for its regularization and accurate prediction of the target variable.\",\"action\":\"Fit the RidgeCV model with the given alphas on the training data using a 5-fold cross-validation. Print out the value of the optimal alpha parameter used in the RidgeCV model.\",\"state\":\"The predictive capability of the model is known through the calculation of the RMSE. Moreover, the RidgeCV model is now set with the optimal alpha value, optimizing its performance.\"}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Training and scoring a Ridge regression model\", \"We need to train a model on the training data and evaluate its performance on the validation data to see how well it can predict the target variable. If we do not do that, we will not be able to assess the model's performance and make improvements\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f4fc3ffe-15d6-4e31-a2d7-6c53b7a938e9",
        "_uuid": "696ec1bb61f908c07dd7508f4809309b91b025c1"
      },
      "outputs": [],
      "source": [
        "rdg = Ridge(alpha=10)\n",
        "rdg.fit(X_train, y_train)\n",
        "rdg.score(X_valid, y_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Calculating the root mean squared error for the model's predictions\", \"We need to evaluate the performance of the model to understand how well it is predicting the target variable. Without this, we cannot determine if the model is performing well or not.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\"title\": \"Train and Score Ridge Model\",\"issue\":\"There is a gap in the process of assessing the performance of the machine learning model on the validation data. We have not yet trained and scored a Ridge regression model on the training and validation datasets respectively.\",\"action\":\"We will do this by training a Ridge regression model with an alpha value of 10 on the training data using X_train and y_train. Then, we will evaluate its performance on the validation data by calculating the score of the model using X_valid and y_valid.\",\"state\":\"The optimal alpha value for the RidgeCV model has been identified and printed out. The model is now properly regularized and ready for accurate predictions.\"}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "65b7e898-59d9-409d-9479-7d93054a841f",
        "_uuid": "d9c10b622561615596b9eb37e05b5e11055a5d52"
      },
      "outputs": [],
      "source": [
        "y_pred = rdg.predict(X_valid)\n",
        "rmse = np.sqrt(metrics.mean_squared_error(y_pred, y_valid))\n",
        "rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Training a Random Forest Regressor\", \"We need to train a model to make predictions on the test data. Without training a model, we cannot make any predictions\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2bd67b78-10bf-4793-a132-b50963b6b470",
        "_uuid": "319e768779413b54d380439babd7af1ec18092e1"
      },
      "outputs": [],
      "source": [
        "rfr = RandomForestRegressor(n_jobs=-1, n_estimators=100)\n",
        "rfr.fit(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Evaluating the Random Forest Regressor model's performance on the validation set\", \"We need to evaluate the model's performance to understand how well it can predict the target variable on unseen data. Without this evaluation, we cannot determine if the model is overfitting or underfitting and make necessary adjustments\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0b81d777-fb08-408d-8cae-c52af5a54185",
        "_uuid": "9632e49ae158c188b01ed8a7d1a6dea23e119f20"
      },
      "outputs": [],
      "source": [
        "rfr.score(X_valid,y_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\"title\": \"Train and Evaluate Random Forest Model\",\"issue\":\"The Random Forest Regressor model, which is crucial for predicting the target variable, has not been trained on the dataset and evaluated on the validation set.\",\"action\":\"Train a Random Forest Regressor on the dataset and predict the target variable for the validation set. Then, evaluate the model's performance on the validation set by calculating the Root Mean Squared Error (RMSE) and the score.\",\"state\":\"The Ridge regression model will now be trained on the training data and evaluated on the validation data. The performance of the model will be clear once the score has been calculated.\"}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Evaluating the Random Forest Regressor model\", \"We need to evaluate the model's performance to understand how well it predicts the target variable. Without this step, we cannot determine if the model is good or not\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "557d53cd-c341-41d3-8ab4-a2bb90a2ce5f",
        "_uuid": "3770eb3c5428d959b991caf33ad5b355b392f492"
      },
      "outputs": [],
      "source": [
        "y_pred = rfr.predict(X_valid)\n",
        "rmse = np.sqrt(metrics.mean_squared_error(y_pred, y_valid))\n",
        "rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Creating a Design Matrix for a Classification Model\", \"We need to convert the categorical variables into a format that can be used by a machine learning model. Without this step, the model would not be able to interpret the categorical data and would not be able to make accurate predictions\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9b4ec272-6829-4167-8b07-d3e4f165fae3",
        "_uuid": "30c74495f335fc0c6d7cd4febad4aecc62005495"
      },
      "outputs": [],
      "source": [
        "import patsy\n",
        "\n",
        "train_df['above_200k'] = train_df['above_200k'].astype(float)\n",
        "formula = 'above_200k ~ %s' % (' + '.join(feature_cols)) \n",
        "y_cls, X_cls = patsy.dmatrices(formula, train_df, return_type='dataframe')\n",
        "print(y.head(2),'\\n\\n', X.head(2))\n",
        "\n",
        "\n",
        "X_cls_train, X_cls_valid, y_cls_train, y_cls_valid = train_test_split(X_cls,y_cls, test_size=0.2)\n",
        "print(X_cls_train.shape, X_cls_valid.shape, y_cls_train.shape, y_valid.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\"title\": \"Evaluate and Transform Random Forest Model\",\"issue\":\"Lack of model evaluation for the Random Forest Regressor on the validation set, which is necessary for understanding the model's prediction accuracy. In addition, the categorical data isn't transformed into a format suitable for a machine learning model, which can lead to poor model understanding and less accurate predictions.\",\"action\":\"Evaluating the performance of the Random Forest Regressor model on the validation set by predicting with the model, then calculating the root mean square error, which assesses the difference between predicted values (y_pred) and the 'true' target values (y_valid) using mean squared error (mse) then taking the 'square root' of it. And we convert categorical variables into the suitable format for the 'scikit-learn' classification model using 'patsy' library, then we make 'scikit-learn' structure for training and testing. \",\"state\":\"The Random Forest Regressor has been trained and used to predict the target variable for the validation set. The model's performance has been evaluated using the RMSE and the score, thereby allowing us to understand its predictive accuracy and make any necessary adjustments to the model.\"}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Training a logistic regression model\", \"We need to train a model to make predictions on the test data. Without training a model, we cannot make any predictions\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "22682131-b81a-461e-a08c-37eef11d2c1c",
        "_uuid": "c363135c8b473f53ab8f9cab6515bb749e5237c5"
      },
      "outputs": [],
      "source": [
        "lgm = LogisticRegression()\n",
        "lgm.fit(X_cls_train,y_cls_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Evaluating model performance\", \"We need to evaluate the performance of the model to understand how well it is predicting the target variable. Without this step, we cannot determine if the model is good enough to make accurate predictions\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\"title\": \"Train and Evaluate Logistic Regression Model\",\"issue\":\"The logistic regression model has not been trained and evaluated using the training dataset.\",\"action\":\"Train a logistic regression model using the training dataset and check the model's performance.\",\"state\":\"Model evaluation is completed by calculating the rmse, which shows the model's prediction performance. Categorical data is converted into a suitable input format, the data is now ready for training or testing with the scikit-learn classification model.\"}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f16a3128-1e06-4da4-99cd-bd7d128a4f14",
        "_uuid": "868e27fe26be45bb3f74136c12123b79f18aab7c"
      },
      "outputs": [],
      "source": [
        "lgm.score(X_cls_valid,y_cls_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Generating confusion matrix for model prediction\", \"We need to evaluate the performance of the model by visualizing the confusion matrix. Without this, we cannot understand the model's accuracy and where it might be making mistakes\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7cc84ccc-308d-4c8c-b579-451c06bbde1e",
        "_uuid": "25318c1488a2303799c3e15bd157a56758f431e6"
      },
      "outputs": [],
      "source": [
        "y_cls_pred = lgm.predict(X_cls_valid)\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    \n",
        "    \n",
        "cnf_matrix = metrics.confusion_matrix(y_cls_valid, y_cls_pred)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=['above_200k', 'below_200k'],\n",
        "                      title='Confusion matrix, without normalization')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Creating a Random Forest Regressor\", \"We need a model to train on our data. Without a model, we cannot make predictions or analyze the data\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c4f3ebce-2859-4510-9fd0-b23bd3fbd966",
        "_uuid": "faf2fe1cd6f7fb685483d9efc5b9546fb06e6edc",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "rfr = RandomForestRegressor(n_jobs=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Performing Grid Search for Random Forest Regressor\", \"We need to find the best parameters for our Random Forest Regressor model to optimize its performance. Without this, we might not achieve the best possible accuracy for our model\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "048e88c3-8e56-427b-a4d8-84183956e18f",
        "_uuid": "cf81f536a58089f4775ccdd25196de4cc5867563"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    'max_features': [0.25, 0.5, 0.7],\n",
        "    'max_depth' : [ 2,5,10,20]\n",
        "}\n",
        "gs = GridSearchCV(cv=5, param_grid=params, estimator=rfr, verbose=0)\n",
        "gs.fit(X_train,y_train.LogSalePrice.ravel())\n",
        "print(gs.best_params_, gs.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Performing randomized search for optimal hyperparameters\", \"We need to find the best hyperparameters for our model to improve its performance. Without this, the model may not perform well on unseen data\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6249b642-1ea2-4ba0-b6f2-8d6f2dc25081",
        "_uuid": "0fc3f25a47ff2fda296f7892bf4d3b67f31bbd6d"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    'max_features': [0.25, 0.5, 0.7],\n",
        "    'max_depth' : [ 2,5,10,20]\n",
        "}\n",
        "rs = RandomizedSearchCV(cv=5, param_distributions=params, estimator=rfr, verbose=0)\n",
        "rs.fit(X_train,y_train.LogSalePrice.ravel())\n",
        "print(rs.best_params_, rs.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Printing the coefficients of linear models\", \"We need to check the coefficients of the models to understand the impact of each feature on the predictions. Without this, we cannot interpret the models and their results\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\"title\": \"Analyze Model Performance with Confusion Matrix\",\"issue\":\"The performance of the model is not adequately analyzed without a confusion matrix which clearly depicts the precision and error rates of the model.\",\"action\":\"Generate and visualize a confusion matrix to evaluate the model's performance. This will include generating the matrix using the predicted and true validation labels, normalizing the data, and visualizing the matrix. The matrix should have functionality to display the outcomes in a normalized or non-normalized format.\",\"state\":\"The logistic regression model has been trained and its performance has been evaluated using the training dataset.\"}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "85283aa0-ab04-46cb-b368-c64283c386b3",
        "_uuid": "b9a719a80a85de53b7fd48b5a77df34c1e16fbac"
      },
      "outputs": [],
      "source": [
        "print(lm.coef_)\n",
        "print(rdgCV.coef_)\n",
        "print(lgm.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Training and evaluating machine learning models\", \"The purpose of this code is to train three different regression models (Linear Regression, RidgeCV, and Random Forest) on the training data and evaluate their performance on both the training and validation sets. Without this code, we would not be able to compare the performance of different models and select the best one for our task.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "20166d61-7bf0-43c7-8752-abb164be351f",
        "_uuid": "a0bd553bffb636a04c4bd77e378077100c7d729d"
      },
      "outputs": [],
      "source": [
        "rfr = RandomForestRegressor(n_jobs=-1, n_estimators=100)\n",
        "rfr.fit(X_train,y_train)\n",
        "\n",
        "y_lm_pred = lm.predict(X_train)\n",
        "y_rdgCV_pred = rdgCV.predict(X_train)\n",
        "y_rfr_pred = rfr.predict(X_train)\n",
        "\n",
        "print('-----training score ---')\n",
        "print(lm.score(X_train, y_train))\n",
        "print(rdgCV.score(X_train, y_train))\n",
        "print(rfr.score(X_train, y_train))\n",
        "print('----Validation score ---')\n",
        "print(lm.score(X_valid, y_valid))\n",
        "print(rdgCV.score(X_valid, y_valid))\n",
        "print(rfr.score(X_valid, y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Extracting the target variable values\", \"We need to separate the target variable from the training data to use it in the machine learning model. Without this step, we cannot train the model to predict the target variable\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\"title\": \"Train and Evaluate Regression Models\",\"issue\":\"Machine learning models have not been trained on the training data and thus, performance cannot be evaluated. Without this, we are unable to select the best predictive model.\",\"action\":\"Train three regression models (Linear Regression, RidgeCV, and Random Forest) on the training data and predict on both the training and validation sets. Obtain the coefficients of all the models and evaluate the performance of each model using the models score method.\",\"state\":\"After generating and visualizing the confusion matrix, we can effectively discern whether we can rely on the model's predictions by simply comparing the true positive and false negative rates present in the confusion matrix'\"}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c1f16c39-270c-4d17-bb9d-cb71d5db49b5",
        "_uuid": "9bc31ae98a72657426c15fce45118fe258c55610"
      },
      "outputs": [],
      "source": [
        "y_cls_train['above_200k'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Generating predictions and probabilities for logistic regression model\", \"We need to evaluate the performance of the model by comparing the predictions with the actual values. Without this step, we cannot assess the accuracy of the model\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "182c1a09-fe7e-41bb-a2ad-649acb35bcf5",
        "_uuid": "e434a1ce22f8cfb5ba666c01d8bf312277336559",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "y_lgm_p = lgm.predict(X_cls_train)\n",
        "y_lgm_lpr = lgm.predict_log_proba(X_cls_train)\n",
        "y_lgm_pr = lgm.predict_proba(X_cls_train)\n",
        "\n",
        "y_lgm_lpr[:,0]\n",
        "y_lgm_pr[:,0]\n",
        "y_lgm_pr[:,1]\n",
        "pd.DataFrame({'true': y_cls_train['above_200k'].values,\n",
        "              'predict':y_lgm_p, \n",
        "              'log_prob_0':y_lgm_lpr[:,0],\n",
        "              'log_prob_1':y_lgm_lpr[:,1],\n",
        "              'prob_0': y_lgm_pr[:,0],\n",
        "              'prob_1': y_lgm_pr[:,1]\n",
        "             }).head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \"Evaluating Model Performance\", \"We need to assess the performance of the model using various metrics to understand its accuracy and reliability. Without this step, we cannot determine if the model is suitable for the task at hand\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\"title\": \"Evaluate Logistic Regression Model Performance\",\"issue\":\"There is no process in place to evaluate the performance of the logistic regression model by using the training data to derive probabilities, predictions, and to compare the predictions against the actual target values.\",\"action\":\"Use the trained logistic regression model lgm to make predictions for the training data (y_lgm_p) and to produce probability scores (y_lgm_dc_pr, y_lgm_lpr). Also, compare the actual target values with the predictions in a DataFrame.\",\"state\":\"Models are trained, and the performance of each model on the training and validation dataset is determined. The coefficients of each regression model are printed, and the score for each model on both datasets is obtained.\"}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\"title\": \"Load Black Friday Sales Dataset\",\"issue\":\"The current DataFrame 'df' does not contain the necessary sales data for the analysis.\",\"action\":\"Load the Black Friday Sales dataset from the file path 'data/black_friday_sales.csv' using pandas.read_csv into a DataFrame.\",\"state\":\"The logistic regression model generates predictions and probabilities for the training data, and these outcomes are compared with the actual values in a DataFrame.\"}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, auc, classification_report, \\\n",
        "confusion_matrix, f1_score, log_loss, precision_recall_curve, roc_auc_score, roc_curve\n",
        "\n",
        "print('Log Loss: ', log_loss(y_lgm_p, y_cls_train))\n",
        "print('Accuracy_score: ', accuracy_score(y_lgm_p, y_cls_train))\n",
        "print('confusion_matrix: ', confusion_matrix(y_lgm_p, y_cls_train))\n",
        "print('Classification_Report: ', classification_report(y_lgm_p, y_cls_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SEP17\n",
        "```json\n",
        "{\"title\": \"Evaluate Logistic Regression Model Performance\",\"issue\":\"There is a need to evaluate the performance of a logistic regression model that has been applied to predict a binary outcome. The outcome predictions and training class labels are stored in 'y_lgm_p' and 'y_cls_train', respectively. There is currently no visual or numerical representation of the models performance with the classification report and confusion matrix.\",\"action\":\"The performance metrics of the model after logistic regression will be reported using precision, recall, F1 score, accuracy, log loss, and AUC values with the classification report and confusion matrix for analysis. The following tools and methods will be used for this action: sklearn.metrics.accuracy_score, sklearn.metrics.classification_report, sklearn.metrics.confusion_matrix, and sklearn.metrics.log_loss.\",\"state\":\"The DataFrame 'df' will be populated with the sales data from Black Friday and ready for data analysis.\"}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " The code snippet provided is an empty cell in a Jupyter notebook. There is no action or purpose to generate as no code is present in the cell. If code were to be added, the name of the action and its purpose would depend on the specific code written. Without code, there is no action or purpose to describe. \"Empty cell in Jupyter notebook\", \"There is no purpose to describe as no code is present in the cell\". If code were to be added, the name of the action and its purpose would depend on the specific code written. Without code, there is no action or purpose to describe. \"Empty cell in Jupyter notebook\", \"There is no purpose to describe as no code is present in the cell\". If code were to be added, the name of the action and its purpose would depend on the specific code written. Without code, there is no action or purpose to describe. \n",
        "\n",
        "Since the provided code snippet is an empty cell in a Jupyter notebook, there is no action or purpose to generate. If code were to be added, the name of the action and its purpose would depend on the specific code written. Without code, there is no action or purpose to describe. \n",
        "\n",
        "Therefore, the answer is: \"Empty cell in J"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
